{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Trainer (has labels)\n",
    "trainer_df = pd.read_csv('datasets/cleaned_SOLIDtest6K_trainer.tsv', sep=\"\\t\")\n",
    "learner_tweets_df = pd.read_csv('datasets/cleaned_SOLID9M_learner.tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# trainer_labels = trainer_df['label'].values\n",
    "# trainer_tweets = trainer_df['tweet'].values\n",
    "learner_tweets_df['labels'] = learner_tweets_df['average'].apply(lambda x: 1 if x >= 0.8 else 0) # threshold the average values\n",
    "\n",
    "sample_size = 80000\n",
    "positive_ratio = 0.75\n",
    "\n",
    "# Select the most confident positive values\n",
    "semi_tweets_pos_df = learner_tweets_df[learner_tweets_df['average'] > 0.8].sample(n=np.floor(sample_size*positive_ratio).astype(int), random_state=1)\n",
    "\n",
    "# Select the most confident negative values\n",
    "semi_tweets_neg_df = learner_tweets_df[learner_tweets_df['average'] < 0.2].sample(n=np.floor(sample_size*(1-positive_ratio)).astype(int), random_state=1)\n",
    "\n",
    "semi_tweets_df = pd.concat([semi_tweets_pos_df, semi_tweets_neg_df])\n",
    "semi_tweets_df = semi_tweets_df.sample(frac=1, random_state=42)\n",
    "\n",
    "semi_tweets = semi_tweets_df['text'].values\n",
    "semi_labels = semi_tweets_df['labels'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>average</th>\n",
       "      <th>std</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2348858</th>\n",
       "      <td>1140493726000787462</td>\n",
       "      <td>cannot no nigga get next to me like i really d...</td>\n",
       "      <td>0.826405</td>\n",
       "      <td>0.162585</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389301</th>\n",
       "      <td>1160578706026270720</td>\n",
       "      <td>this movie used to fuck me up</td>\n",
       "      <td>0.827979</td>\n",
       "      <td>0.167209</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7847151</th>\n",
       "      <td>1188696905586528256</td>\n",
       "      <td>my eye was fascinated by that memorandum</td>\n",
       "      <td>0.196262</td>\n",
       "      <td>0.158945</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2782796</th>\n",
       "      <td>1161519296075505665</td>\n",
       "      <td>oh well thats good sleep well then</td>\n",
       "      <td>0.182599</td>\n",
       "      <td>0.193224</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400928</th>\n",
       "      <td>1160317975276208128</td>\n",
       "      <td>wearing your butt plug to ur dinner date so he...</td>\n",
       "      <td>0.864163</td>\n",
       "      <td>0.143021</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7010621</th>\n",
       "      <td>1186516651325280256</td>\n",
       "      <td>who the fuck ate all my cum</td>\n",
       "      <td>0.867288</td>\n",
       "      <td>0.165063</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2190200</th>\n",
       "      <td>1159412408521318400</td>\n",
       "      <td>and you know the ish is staged and not genuine...</td>\n",
       "      <td>0.188691</td>\n",
       "      <td>0.168918</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595615</th>\n",
       "      <td>1161920544314294272</td>\n",
       "      <td>adnthweeksary    for us to regard others as wo...</td>\n",
       "      <td>0.153824</td>\n",
       "      <td>0.174742</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5239813</th>\n",
       "      <td>1157885199545319424</td>\n",
       "      <td>ill eat that son of a bitch</td>\n",
       "      <td>0.871429</td>\n",
       "      <td>0.151313</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6244358</th>\n",
       "      <td>1187425309496352769</td>\n",
       "      <td>why you lie to her like that  i fucked some la...</td>\n",
       "      <td>0.813525</td>\n",
       "      <td>0.162951</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id  \\\n",
       "2348858  1140493726000787462   \n",
       "389301   1160578706026270720   \n",
       "7847151  1188696905586528256   \n",
       "2782796  1161519296075505665   \n",
       "400928   1160317975276208128   \n",
       "7010621  1186516651325280256   \n",
       "2190200  1159412408521318400   \n",
       "1595615  1161920544314294272   \n",
       "5239813  1157885199545319424   \n",
       "6244358  1187425309496352769   \n",
       "\n",
       "                                                      text   average  \\\n",
       "2348858  cannot no nigga get next to me like i really d...  0.826405   \n",
       "389301                       this movie used to fuck me up  0.827979   \n",
       "7847151           my eye was fascinated by that memorandum  0.196262   \n",
       "2782796                 oh well thats good sleep well then  0.182599   \n",
       "400928   wearing your butt plug to ur dinner date so he...  0.864163   \n",
       "7010621                        who the fuck ate all my cum  0.867288   \n",
       "2190200  and you know the ish is staged and not genuine...  0.188691   \n",
       "1595615  adnthweeksary    for us to regard others as wo...  0.153824   \n",
       "5239813                        ill eat that son of a bitch  0.871429   \n",
       "6244358  why you lie to her like that  i fucked some la...  0.813525   \n",
       "\n",
       "              std  labels  \n",
       "2348858  0.162585       1  \n",
       "389301   0.167209       1  \n",
       "7847151  0.158945       0  \n",
       "2782796  0.193224       0  \n",
       "400928   0.143021       1  \n",
       "7010621  0.165063       1  \n",
       "2190200  0.168918       0  \n",
       "1595615  0.174742       0  \n",
       "5239813  0.151313       1  \n",
       "6244358  0.162951       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semi_tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            return {\n",
    "                'text': text,\n",
    "                'label': label\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'text': text\n",
    "            }\n",
    "        \n",
    "# trainer_dataset = TweetDataset(trainer_tweets, trainer_labels)\n",
    "learner_dataset = TweetDataset(semi_tweets, semi_labels)\n",
    "\n",
    "# trainer_loader = DataLoader(trainer_dataset, batch_size=12, shuffle=True)\n",
    "learner_loader = DataLoader(learner_dataset, batch_size=6, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('models/OLID_BERT_1', num_labels=2)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13334 [00:00<?, ?it/s]/tmp/ipykernel_9126/1995336368.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(batch['label']).clone().detach().to(device)\n",
      "100%|██████████| 13334/13334 [12:25<00:00, 17.90it/s]\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    # trainer_dataloader_iterator = iter(trainer_loader)\n",
    "    semi_labeled_dataloader_iterator = iter(learner_loader)\n",
    "    num_batches = len(learner_loader)\n",
    "    # num_batches = 5000\n",
    "    for _ in tqdm(range(num_batches)):\n",
    "        # Train on labeled data\n",
    "        batch = next(semi_labeled_dataloader_iterator, None)\n",
    "        if batch is not None:\n",
    "            inputs = tokenizer(batch['text'], padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "            labels = torch.tensor(batch['label']).clone().detach().to(device)\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('models/SOLID_80ksemi_OLIDBERT_3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvidia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
