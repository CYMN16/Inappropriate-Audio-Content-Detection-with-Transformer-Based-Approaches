{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdoga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, ConvBertForSequenceClassification, ConvBertTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load dataset\n",
    "df_olidtest = pd.read_csv('../datasets/cleaned_OLID_test.tsv', sep=\"\\t\")\n",
    "# df = pd.read_csv('datasets/cleaned_OLID.tsv', sep=\"\\t\")\n",
    "df_solid = pd.read_csv('../datasets/cleaned_SOLIDtest6K_trainer.tsv', sep='\\t')\n",
    "df_troff = pd.read_csv('../datasets/cleaned_tr_offenseval_test.tsv', sep='\\t')\n",
    "df_hso = pd.read_csv('../datasets/cleaned_hatespeech_offensive_test.tsv', sep='\\t')\n",
    "\n",
    "# Assuming your columns are named 'tweet' and 'class', change accordingly\n",
    "tweets_olid = df_olidtest['tweet'].values\n",
    "labels_df_olid = df_olidtest['label'].values\n",
    "\n",
    "tweets_solid = df_solid['tweet'].values\n",
    "labels_df_solid = df_solid['label'].values\n",
    "\n",
    "tweets_troff = df_troff['tweet'].values\n",
    "labels_df_troff = df_troff['label'].values\n",
    "\n",
    "tweets_hso = df_hso['tweet'].values\n",
    "labels_df_hso = df_hso['label'].values\n",
    "\n",
    "full_model_name = 'YituTech/conv-bert-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(full_model_name)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-emotion\")\n",
    "\n",
    "# Tokenize and encode the training and validation texts\n",
    "# train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True)\n",
    "encodings_olid = tokenizer(tweets_olid.tolist(), truncation=True, padding=True)\n",
    "encodings_solid = tokenizer(tweets_solid.tolist(), truncation=True, padding=True)\n",
    "encodings_troff = tokenizer(tweets_troff.tolist(), truncation=True, padding=True)\n",
    "encodings_hso = tokenizer(tweets_hso.tolist(), truncation=True, padding=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# train_dataset = TweetDataset(train_encodings, train_labels)\n",
    "val_dataset_olid = TweetDataset(encodings_olid, labels_df_olid)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=12, shuffle=True)\n",
    "val_loader_olid = torch.utils.data.DataLoader(val_dataset_olid, batch_size=12, shuffle=False)\n",
    "\n",
    "\n",
    "val_dataset_solid = TweetDataset(encodings_solid, labels_df_solid)\n",
    "val_loader_solid = torch.utils.data.DataLoader(val_dataset_solid, batch_size=12, shuffle=False)\n",
    "\n",
    "val_dataset_troff = TweetDataset(encodings_troff, labels_df_troff)\n",
    "val_loader_troff = torch.utils.data.DataLoader(val_dataset_troff, batch_size=12, shuffle=False)\n",
    "\n",
    "val_dataset_hso = TweetDataset(encodings_hso, labels_df_hso)\n",
    "val_loader_hso = torch.utils.data.DataLoader(val_dataset_hso, batch_size=12, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from transformers import BertForSequenceClassification, ConvBertForSequenceClassification\n",
    "import time\n",
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "model_olid = BertForSequenceClassification.from_pretrained(f\"models/{model_name}_olid\")\n",
    "model_solid = BertForSequenceClassification.from_pretrained(f\"models/{model_name}_solid\")\n",
    "model_olidsolid = BertForSequenceClassification.from_pretrained(f\"models/{model_name}_olid_solid\")\n",
    "model_solidtroff = BertForSequenceClassification.from_pretrained(f\"models/{model_name}_solid_tr\")\n",
    "model_hso = BertForSequenceClassification.from_pretrained(f\"models/{model_name}_hso\")\n",
    "\n",
    "# Define optimizer and learning rate\n",
    "optimizer = optim.AdamW(model_olid.parameters(), lr=1e-5)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "olid dataset test time:  2.151456356048584  seconds\n",
      "Validation Accuracy: 0.8476744186046512\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.89       620\n",
      "           1       0.73      0.71      0.72       240\n",
      "\n",
      "    accuracy                           0.85       860\n",
      "   macro avg       0.81      0.81      0.81       860\n",
      "weighted avg       0.85      0.85      0.85       860\n",
      "\n",
      "True Positives (TP): 171\n",
      "True Negatives (TN): 558\n",
      "False Positives (FP): 62\n",
      "False Negatives (FN): 69\n"
     ]
    }
   ],
   "source": [
    "## olid model on olid test\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluation\n",
    "model_olid.to(device)\n",
    "model_olid.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_olid:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_olid(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_olid.to('cpu')\n",
    "\n",
    "print(\"olid dataset test time: \", test_end - test_start, \" seconds\")\n",
    "accuracy = correct / total\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_olid, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_olid, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "solid dataset test time:  12.404554843902588  seconds\n",
      "Validation Accuracy: 0.9209077256799599\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.85      0.91      2991\n",
      "           1       0.87      0.99      0.93      3002\n",
      "\n",
      "    accuracy                           0.92      5993\n",
      "   macro avg       0.93      0.92      0.92      5993\n",
      "weighted avg       0.93      0.92      0.92      5993\n",
      "\n",
      "True Positives (TP): 2978\n",
      "True Negatives (TN): 2541\n",
      "False Positives (FP): 450\n",
      "False Negatives (FN): 24\n"
     ]
    }
   ],
   "source": [
    "## olid on solid test\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluation\n",
    "model_olid.to(device)\n",
    "model_olid.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_solid:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_olid(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_olid.to('cpu')\n",
    "\n",
    "print(\"solid dataset test time: \", test_end - test_start, \" seconds\")\n",
    "accuracy = correct / total\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_solid, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_solid, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "hso dataset test time:  11.079460620880127  seconds\n",
      "Validation Accuracy: 0.8384103288279201\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.52      0.67      0.58       835\n",
      "           1       0.93      0.87      0.90      4122\n",
      "\n",
      "    accuracy                           0.84      4957\n",
      "   macro avg       0.72      0.77      0.74      4957\n",
      "weighted avg       0.86      0.84      0.85      4957\n",
      "\n",
      "True Positives (TP): 3596\n",
      "True Negatives (TN): 560\n",
      "False Positives (FP): 275\n",
      "False Negatives (FN): 526\n"
     ]
    }
   ],
   "source": [
    "## olid model on hso test\n",
    "\n",
    "# Evaluation\n",
    "model_olid.to(device)\n",
    "model_olid.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_hso:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_olid(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_olid.to('cpu')\n",
    "\n",
    "print(\"hso dataset test time: \", test_end - test_start, \" seconds\")\n",
    "accuracy = correct / total\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_hso, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_hso, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "olid dataset test time:  2.1589205265045166  seconds\n",
      "Validation Accuracy: 0.8383720930232558\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.89       620\n",
      "           1       0.71      0.72      0.71       240\n",
      "\n",
      "    accuracy                           0.84       860\n",
      "   macro avg       0.80      0.80      0.80       860\n",
      "weighted avg       0.84      0.84      0.84       860\n",
      "\n",
      "True Positives (TP): 173\n",
      "True Negatives (TN): 548\n",
      "False Positives (FP): 72\n",
      "False Negatives (FN): 67\n"
     ]
    }
   ],
   "source": [
    "## solid model on olid test\n",
    "\n",
    "# Evaluation\n",
    "###\n",
    "model_solid.to(device)\n",
    "model_solid.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_olid: ###\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_solid(input_ids, attention_mask=attention_mask) ###\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solid.to('cpu') ###\n",
    "\n",
    "print(\"olid dataset test time: \", test_end - test_start, \" seconds\")\n",
    "accuracy = correct / total\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_olid, prediction_list)) ###\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_olid, prediction_list) ###\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "solid dataset test time:  12.400338172912598  seconds\n",
      "Validation Accuracy: 0.9182379442683131\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.85      0.91      2991\n",
      "           1       0.87      0.99      0.92      3002\n",
      "\n",
      "    accuracy                           0.92      5993\n",
      "   macro avg       0.93      0.92      0.92      5993\n",
      "weighted avg       0.93      0.92      0.92      5993\n",
      "\n",
      "True Positives (TP): 2969\n",
      "True Negatives (TN): 2534\n",
      "False Positives (FP): 457\n",
      "False Negatives (FN): 33\n"
     ]
    }
   ],
   "source": [
    "## solid model on solid test\n",
    "\n",
    "# Evaluation\n",
    "###\n",
    "model_solid.to(device)\n",
    "model_solid.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_solid: ###\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_solid(input_ids, attention_mask=attention_mask) ###\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solid.to('cpu') ###\n",
    "\n",
    "print(\"solid dataset test time: \", test_end - test_start, \" seconds\")\n",
    "accuracy = correct / total\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_solid, prediction_list)) ###\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_solid, prediction_list) ###\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "hso dataset test time:  11.110952615737915  seconds\n",
      "Validation Accuracy: 0.802703247932217\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.68      0.54       835\n",
      "           1       0.93      0.83      0.87      4122\n",
      "\n",
      "    accuracy                           0.80      4957\n",
      "   macro avg       0.69      0.75      0.71      4957\n",
      "weighted avg       0.85      0.80      0.82      4957\n",
      "\n",
      "True Positives (TP): 3410\n",
      "True Negatives (TN): 569\n",
      "False Positives (FP): 266\n",
      "False Negatives (FN): 712\n"
     ]
    }
   ],
   "source": [
    "## solid model on hso test\n",
    "\n",
    "# Evaluation\n",
    "###\n",
    "model_solid.to(device)\n",
    "model_solid.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_hso: ###\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_solid(input_ids, attention_mask=attention_mask) ###\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solid.to('cpu') ###\n",
    "\n",
    "print(\"hso dataset test time: \", test_end - test_start, \" seconds\")\n",
    "accuracy = correct / total\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_hso, prediction_list)) ###\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_hso, prediction_list) ###\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "olid dataset test time:  2.1608002185821533  seconds\n",
      "Validation Accuracy: 0.8244186046511628\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.85      0.87       620\n",
      "           1       0.66      0.76      0.71       240\n",
      "\n",
      "    accuracy                           0.82       860\n",
      "   macro avg       0.78      0.81      0.79       860\n",
      "weighted avg       0.83      0.82      0.83       860\n",
      "\n",
      "True Positives (TP): 183\n",
      "True Negatives (TN): 526\n",
      "False Positives (FP): 94\n",
      "False Negatives (FN): 57\n"
     ]
    }
   ],
   "source": [
    "## olid + solid model on olid test\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_olidsolid.to(device)\n",
    "\n",
    "model_olidsolid.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_olid:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_olidsolid(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_olidsolid.to('cpu')\n",
    "\n",
    "print(\"olid dataset test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_olid, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_olid, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "solid dataset test time:  12.41598916053772  seconds\n",
      "Validation Accuracy: 0.9172367762389454\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.84      0.91      2991\n",
      "           1       0.86      0.99      0.92      3002\n",
      "\n",
      "    accuracy                           0.92      5993\n",
      "   macro avg       0.93      0.92      0.92      5993\n",
      "weighted avg       0.93      0.92      0.92      5993\n",
      "\n",
      "True Positives (TP): 2979\n",
      "True Negatives (TN): 2518\n",
      "False Positives (FP): 473\n",
      "False Negatives (FN): 23\n"
     ]
    }
   ],
   "source": [
    "## olid + solid model on solid test\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_olidsolid.to(device)\n",
    "\n",
    "model_olidsolid.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_solid:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_olidsolid(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_olidsolid.to('cpu')\n",
    "\n",
    "print(\"solid dataset test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_solid, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_solid, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "hso dataset test time:  11.135472297668457  seconds\n",
      "Validation Accuracy: 0.8178333669558201\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.64      0.54       835\n",
      "           1       0.92      0.85      0.89      4122\n",
      "\n",
      "    accuracy                           0.82      4957\n",
      "   macro avg       0.70      0.75      0.71      4957\n",
      "weighted avg       0.85      0.82      0.83      4957\n",
      "\n",
      "True Positives (TP): 3516\n",
      "True Negatives (TN): 538\n",
      "False Positives (FP): 297\n",
      "False Negatives (FN): 606\n"
     ]
    }
   ],
   "source": [
    "## olid + solid model on hso test\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_olidsolid.to(device)\n",
    "\n",
    "model_olidsolid.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_hso:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_olidsolid(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_olidsolid.to('cpu')\n",
    "\n",
    "print(\"hso dataset test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_hso, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_hso, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "olid datset test time:  2.1643564701080322  seconds\n",
      "Validation Accuracy: 0.8337209302325581\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.93      0.89       620\n",
      "           1       0.76      0.59      0.66       240\n",
      "\n",
      "    accuracy                           0.83       860\n",
      "   macro avg       0.81      0.76      0.78       860\n",
      "weighted avg       0.83      0.83      0.83       860\n",
      "\n",
      "True Positives (TP): 141\n",
      "True Negatives (TN): 576\n",
      "False Positives (FP): 44\n",
      "False Negatives (FN): 99\n"
     ]
    }
   ],
   "source": [
    "## solid + troff model on olid test\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_solidtroff.to(device)\n",
    "\n",
    "model_solidtroff.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_olid:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_solidtroff(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solidtroff.to('cpu')\n",
    "\n",
    "print(\"olid datset test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_olid, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_olid, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "solid dataset test time:  12.433947086334229  seconds\n",
      "Validation Accuracy: 0.9175704989154013\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.88      0.91      2991\n",
      "           1       0.89      0.96      0.92      3002\n",
      "\n",
      "    accuracy                           0.92      5993\n",
      "   macro avg       0.92      0.92      0.92      5993\n",
      "weighted avg       0.92      0.92      0.92      5993\n",
      "\n",
      "True Positives (TP): 2878\n",
      "True Negatives (TN): 2621\n",
      "False Positives (FP): 370\n",
      "False Negatives (FN): 124\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## olid + troff model on solid test\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_solidtroff.to(device)\n",
    "\n",
    "model_solidtroff.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_solid:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_solidtroff(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solidtroff.to('cpu')\n",
    "\n",
    "print(\"solid dataset test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_solid, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_solid, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "hso test time:  11.139912605285645  seconds\n",
      "Validation Accuracy: 0.7730482146459552\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.76      0.53       835\n",
      "           1       0.94      0.78      0.85      4122\n",
      "\n",
      "    accuracy                           0.77      4957\n",
      "   macro avg       0.67      0.77      0.69      4957\n",
      "weighted avg       0.85      0.77      0.80      4957\n",
      "\n",
      "True Positives (TP): 3196\n",
      "True Negatives (TN): 636\n",
      "False Positives (FP): 199\n",
      "False Negatives (FN): 926\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## olid + troff model on hso test\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_solidtroff.to(device)\n",
    "\n",
    "model_solidtroff.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_hso:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_solidtroff(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solidtroff.to('cpu')\n",
    "\n",
    "print(\"hso test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_hso, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_hso, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "troff test time:  38.539230823516846  seconds\n",
      "Validation Accuracy: 0.8512091038406828\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.99      0.91      2804\n",
      "           1       0.90      0.30      0.45       711\n",
      "\n",
      "    accuracy                           0.85      3515\n",
      "   macro avg       0.87      0.64      0.68      3515\n",
      "weighted avg       0.86      0.85      0.82      3515\n",
      "\n",
      "True Positives (TP): 211\n",
      "True Negatives (TN): 2781\n",
      "False Positives (FP): 23\n",
      "False Negatives (FN): 500\n"
     ]
    }
   ],
   "source": [
    "## solid + troff model on troff test\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_solidtroff.to(device)\n",
    "\n",
    "model_solidtroff.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_troff:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_solidtroff(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solidtroff.to('cpu')\n",
    "\n",
    "print(\"troff test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_troff, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_troff, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "olid test time:  2.1615843772888184  seconds\n",
      "Validation Accuracy: 0.8395348837209302\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.94      0.89       620\n",
      "           1       0.78      0.59      0.67       240\n",
      "\n",
      "    accuracy                           0.84       860\n",
      "   macro avg       0.82      0.76      0.78       860\n",
      "weighted avg       0.83      0.84      0.83       860\n",
      "\n",
      "True Positives (TP): 141\n",
      "True Negatives (TN): 581\n",
      "False Positives (FP): 39\n",
      "False Negatives (FN): 99\n"
     ]
    }
   ],
   "source": [
    "## hsomodel on olid test\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_hso.to(device)\n",
    "\n",
    "model_hso.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_olid:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_hso(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solidtroff.to('cpu')\n",
    "\n",
    "print(\"olid test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_olid, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_olid, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "solid test time:  12.5849027633667  seconds\n",
      "Validation Accuracy: 0.9154013015184381\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.88      0.91      2991\n",
      "           1       0.89      0.95      0.92      3002\n",
      "\n",
      "    accuracy                           0.92      5993\n",
      "   macro avg       0.92      0.92      0.92      5993\n",
      "weighted avg       0.92      0.92      0.92      5993\n",
      "\n",
      "True Positives (TP): 2864\n",
      "True Negatives (TN): 2622\n",
      "False Positives (FP): 369\n",
      "False Negatives (FN): 138\n"
     ]
    }
   ],
   "source": [
    "## hsomodel on solid test\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_hso.to(device)\n",
    "\n",
    "model_hso.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_solid:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_hso(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solidtroff.to('cpu')\n",
    "\n",
    "print(\"solid test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_solid, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_solid, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "olid+solid test time:  11.167417764663696  seconds\n",
      "Validation Accuracy: 0.7734516844865846\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.41      0.76      0.53       835\n",
      "           1       0.94      0.78      0.85      4122\n",
      "\n",
      "    accuracy                           0.77      4957\n",
      "   macro avg       0.67      0.77      0.69      4957\n",
      "weighted avg       0.85      0.77      0.80      4957\n",
      "\n",
      "True Positives (TP): 3196\n",
      "True Negatives (TN): 638\n",
      "False Positives (FP): 197\n",
      "False Negatives (FN): 926\n"
     ]
    }
   ],
   "source": [
    "## hsomodel on hso test\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_hso.to(device)\n",
    "\n",
    "model_hso.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_hso:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_hso(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solidtroff.to('cpu')\n",
    "\n",
    "print(\"olid+solid test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_hso, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_hso, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
