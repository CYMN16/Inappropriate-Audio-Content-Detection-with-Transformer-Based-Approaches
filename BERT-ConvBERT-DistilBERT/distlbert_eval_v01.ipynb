{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdoga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, ConvBertForSequenceClassification, ConvBertTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load dataset\n",
    "df_olidtest = pd.read_csv('../datasets/cleaned_OLID_test.tsv', sep=\"\\t\")\n",
    "# df = pd.read_csv('datasets/cleaned_OLID.tsv', sep=\"\\t\")\n",
    "df_solid = pd.read_csv('../datasets/cleaned_SOLIDtest6K_trainer.tsv', sep='\\t')\n",
    "df_troff = pd.read_csv('../datasets/cleaned_tr_offenseval_test.tsv', sep='\\t')\n",
    "df_hso = pd.read_csv('../datasets/cleaned_hatespeech_offensive_test.tsv', sep='\\t')\n",
    "\n",
    "# Assuming your columns are named 'tweet' and 'class', change accordingly\n",
    "tweets_olid = df_olidtest['tweet'].values\n",
    "labels_df_olid = df_olidtest['label'].values\n",
    "\n",
    "tweets_solid = df_solid['tweet'].values\n",
    "labels_df_solid = df_solid['label'].values\n",
    "\n",
    "tweets_troff = df_troff['tweet'].values\n",
    "labels_df_troff = df_troff['label'].values\n",
    "\n",
    "tweets_hso = df_hso['tweet'].values\n",
    "labels_df_hso = df_hso['label'].values\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\") ## bu ve alttaki değişiyor, birde buna göre importlar değişir\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-emotion\")\n",
    "\n",
    "# Tokenize and encode the training and validation texts\n",
    "# train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True)\n",
    "encodings_olid = tokenizer(tweets_olid.tolist(), truncation=True, padding=True)\n",
    "encodings_solid = tokenizer(tweets_solid.tolist(), truncation=True, padding=True)\n",
    "encodings_troff = tokenizer(tweets_troff.tolist(), truncation=True, padding=True)\n",
    "encodings_hso = tokenizer(tweets_hso.tolist(), truncation=True, padding=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# train_dataset = TweetDataset(train_encodings, train_labels)\n",
    "val_dataset_olid = TweetDataset(encodings_olid, labels_df_olid)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=12, shuffle=True)\n",
    "val_loader_olid = torch.utils.data.DataLoader(val_dataset_olid, batch_size=12, shuffle=False)\n",
    "\n",
    "\n",
    "val_dataset_solid = TweetDataset(encodings_solid, labels_df_solid)\n",
    "val_loader_solid = torch.utils.data.DataLoader(val_dataset_solid, batch_size=12, shuffle=False)\n",
    "\n",
    "val_dataset_troff = TweetDataset(encodings_troff, labels_df_troff)\n",
    "val_loader_troff = torch.utils.data.DataLoader(val_dataset_troff, batch_size=12, shuffle=False)\n",
    "\n",
    "val_dataset_hso = TweetDataset(encodings_hso, labels_df_hso)\n",
    "val_loader_hso = torch.utils.data.DataLoader(val_dataset_hso, batch_size=12, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from transformers import BertForSequenceClassification, DistilBertForSequenceClassification\n",
    "import time\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "model_olid = DistilBertForSequenceClassification.from_pretrained(f\"models/{model_name}_olid\")\n",
    "model_solid = DistilBertForSequenceClassification.from_pretrained(f\"models/{model_name}_solid\")\n",
    "model_olidsolid = DistilBertForSequenceClassification.from_pretrained(f\"models/{model_name}_olid_solid\")\n",
    "model_solidtroff = DistilBertForSequenceClassification.from_pretrained(f\"models/{model_name}_solid_tr\")\n",
    "model_hso = DistilBertForSequenceClassification.from_pretrained(f\"models/{model_name}_hso\")\n",
    "\n",
    "# Define optimizer and learning rate\n",
    "optimizer = optim.AdamW(model_olid.parameters(), lr=1e-5)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "olid dataset test time:  1.2041773796081543  seconds\n",
      "Validation Accuracy: 0.8465116279069768\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.89       620\n",
      "           1       0.73      0.70      0.72       240\n",
      "\n",
      "    accuracy                           0.85       860\n",
      "   macro avg       0.81      0.80      0.81       860\n",
      "weighted avg       0.84      0.85      0.85       860\n",
      "\n",
      "True Positives (TP): 169\n",
      "True Negatives (TN): 559\n",
      "False Positives (FP): 61\n",
      "False Negatives (FN): 71\n"
     ]
    }
   ],
   "source": [
    "## olid model on olid test\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluation\n",
    "model_olid.to(device)\n",
    "model_olid.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_olid:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_olid(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_olid.to('cpu')\n",
    "\n",
    "print(\"olid dataset test time: \", test_end - test_start, \" seconds\")\n",
    "accuracy = correct / total\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_olid, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_olid, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "solid dataset test time:  6.376095533370972  seconds\n",
      "Validation Accuracy: 0.9187385282829968\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.85      0.91      2991\n",
      "           1       0.87      0.99      0.92      3002\n",
      "\n",
      "    accuracy                           0.92      5993\n",
      "   macro avg       0.93      0.92      0.92      5993\n",
      "weighted avg       0.93      0.92      0.92      5993\n",
      "\n",
      "True Positives (TP): 2974\n",
      "True Negatives (TN): 2532\n",
      "False Positives (FP): 459\n",
      "False Negatives (FN): 28\n"
     ]
    }
   ],
   "source": [
    "## olid on solid test\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluation\n",
    "model_olid.to(device)\n",
    "model_olid.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_solid:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_olid(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_olid.to('cpu')\n",
    "\n",
    "print(\"solid dataset test time: \", test_end - test_start, \" seconds\")\n",
    "accuracy = correct / total\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_solid, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_solid, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "hso dataset test time:  5.6520795822143555  seconds\n",
      "Validation Accuracy: 0.8107726447448054\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.67      0.55       835\n",
      "           1       0.93      0.84      0.88      4122\n",
      "\n",
      "    accuracy                           0.81      4957\n",
      "   macro avg       0.69      0.76      0.71      4957\n",
      "weighted avg       0.85      0.81      0.82      4957\n",
      "\n",
      "True Positives (TP): 3457\n",
      "True Negatives (TN): 562\n",
      "False Positives (FP): 273\n",
      "False Negatives (FN): 665\n"
     ]
    }
   ],
   "source": [
    "## olid model on hso test\n",
    "\n",
    "# Evaluation\n",
    "model_olid.to(device)\n",
    "model_olid.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_hso:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_olid(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_olid.to('cpu')\n",
    "\n",
    "print(\"hso dataset test time: \", test_end - test_start, \" seconds\")\n",
    "accuracy = correct / total\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_hso, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_hso, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "olid dataset test time:  2.665764331817627  seconds\n",
      "Validation Accuracy: 0.827906976744186\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.87      0.88       620\n",
      "           1       0.68      0.72      0.70       240\n",
      "\n",
      "    accuracy                           0.83       860\n",
      "   macro avg       0.79      0.80      0.79       860\n",
      "weighted avg       0.83      0.83      0.83       860\n",
      "\n",
      "True Positives (TP): 173\n",
      "True Negatives (TN): 539\n",
      "False Positives (FP): 81\n",
      "False Negatives (FN): 67\n"
     ]
    }
   ],
   "source": [
    "## solid model on olid test\n",
    "\n",
    "# Evaluation\n",
    "###\n",
    "model_solid.to(device)\n",
    "model_solid.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_olid: ###\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_solid(input_ids, attention_mask=attention_mask) ###\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solid.to('cpu') ###\n",
    "\n",
    "print(\"olid dataset test time: \", test_end - test_start, \" seconds\")\n",
    "accuracy = correct / total\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_olid, prediction_list)) ###\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_olid, prediction_list) ###\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "solid dataset test time:  15.686945915222168  seconds\n",
      "Validation Accuracy: 0.9140664108126147\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.85      0.91      2991\n",
      "           1       0.86      0.98      0.92      3002\n",
      "\n",
      "    accuracy                           0.91      5993\n",
      "   macro avg       0.92      0.91      0.91      5993\n",
      "weighted avg       0.92      0.91      0.91      5993\n",
      "\n",
      "True Positives (TP): 2947\n",
      "True Negatives (TN): 2531\n",
      "False Positives (FP): 460\n",
      "False Negatives (FN): 55\n"
     ]
    }
   ],
   "source": [
    "## solid model on solid test\n",
    "\n",
    "# Evaluation\n",
    "###\n",
    "model_solid.to(device)\n",
    "model_solid.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_solid: ###\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_solid(input_ids, attention_mask=attention_mask) ###\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solid.to('cpu') ###\n",
    "\n",
    "print(\"solid dataset test time: \", test_end - test_start, \" seconds\")\n",
    "accuracy = correct / total\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_solid, prediction_list)) ###\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_solid, prediction_list) ###\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "hso dataset test time:  13.469242334365845  seconds\n",
      "Validation Accuracy: 0.8022997780915877\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.72      0.55       835\n",
      "           1       0.94      0.82      0.87      4122\n",
      "\n",
      "    accuracy                           0.80      4957\n",
      "   macro avg       0.69      0.77      0.71      4957\n",
      "weighted avg       0.85      0.80      0.82      4957\n",
      "\n",
      "True Positives (TP): 3372\n",
      "True Negatives (TN): 605\n",
      "False Positives (FP): 230\n",
      "False Negatives (FN): 750\n"
     ]
    }
   ],
   "source": [
    "## solid model on hso test\n",
    "\n",
    "# Evaluation\n",
    "###\n",
    "model_solid.to(device)\n",
    "model_solid.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_hso: ###\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_solid(input_ids, attention_mask=attention_mask) ###\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solid.to('cpu') ###\n",
    "\n",
    "print(\"hso dataset test time: \", test_end - test_start, \" seconds\")\n",
    "accuracy = correct / total\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_hso, prediction_list)) ###\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_hso, prediction_list) ###\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "olid dataset test time:  2.7018203735351562  seconds\n",
      "Validation Accuracy: 0.8151162790697675\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.85      0.87       620\n",
      "           1       0.65      0.72      0.69       240\n",
      "\n",
      "    accuracy                           0.82       860\n",
      "   macro avg       0.77      0.79      0.78       860\n",
      "weighted avg       0.82      0.82      0.82       860\n",
      "\n",
      "True Positives (TP): 173\n",
      "True Negatives (TN): 528\n",
      "False Positives (FP): 92\n",
      "False Negatives (FN): 67\n"
     ]
    }
   ],
   "source": [
    "## olid + solid model on olid test\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_olidsolid.to(device)\n",
    "\n",
    "model_olidsolid.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_olid:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_olidsolid(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_olidsolid.to('cpu')\n",
    "\n",
    "print(\"olid dataset test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_olid, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_olid, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "solid dataset test time:  15.448938846588135  seconds\n",
      "Validation Accuracy: 0.9138995494743868\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.84      0.91      2991\n",
      "           1       0.86      0.99      0.92      3002\n",
      "\n",
      "    accuracy                           0.91      5993\n",
      "   macro avg       0.92      0.91      0.91      5993\n",
      "weighted avg       0.92      0.91      0.91      5993\n",
      "\n",
      "True Positives (TP): 2973\n",
      "True Negatives (TN): 2504\n",
      "False Positives (FP): 487\n",
      "False Negatives (FN): 29\n"
     ]
    }
   ],
   "source": [
    "## olid + solid model on solid test\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_olidsolid.to(device)\n",
    "\n",
    "model_olidsolid.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_solid:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_olidsolid(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_olidsolid.to('cpu')\n",
    "\n",
    "print(\"solid dataset test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_solid, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_solid, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "hso dataset test time:  13.50259518623352  seconds\n",
      "Validation Accuracy: 0.8059310066572524\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.67      0.54       835\n",
      "           1       0.93      0.83      0.88      4122\n",
      "\n",
      "    accuracy                           0.81      4957\n",
      "   macro avg       0.69      0.75      0.71      4957\n",
      "weighted avg       0.85      0.81      0.82      4957\n",
      "\n",
      "True Positives (TP): 3435\n",
      "True Negatives (TN): 560\n",
      "False Positives (FP): 275\n",
      "False Negatives (FN): 687\n"
     ]
    }
   ],
   "source": [
    "## olid + solid model on hso test\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_olidsolid.to(device)\n",
    "\n",
    "model_olidsolid.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_hso:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_olidsolid(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_olidsolid.to('cpu')\n",
    "\n",
    "print(\"hso dataset test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_hso, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_hso, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "olid datset test time:  2.6847922801971436  seconds\n",
      "Validation Accuracy: 0.8395348837209302\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.92      0.89       620\n",
      "           1       0.75      0.64      0.69       240\n",
      "\n",
      "    accuracy                           0.84       860\n",
      "   macro avg       0.81      0.78      0.79       860\n",
      "weighted avg       0.83      0.84      0.84       860\n",
      "\n",
      "True Positives (TP): 153\n",
      "True Negatives (TN): 569\n",
      "False Positives (FP): 51\n",
      "False Negatives (FN): 87\n"
     ]
    }
   ],
   "source": [
    "## solid + troff model on olid test\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_solidtroff.to(device)\n",
    "\n",
    "model_solidtroff.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_olid:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_solidtroff(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solidtroff.to('cpu')\n",
    "\n",
    "print(\"olid datset test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_olid, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_olid, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "solid dataset test time:  15.760691165924072  seconds\n",
      "Validation Accuracy: 0.9147338561655265\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.86      0.91      2991\n",
      "           1       0.88      0.96      0.92      3002\n",
      "\n",
      "    accuracy                           0.91      5993\n",
      "   macro avg       0.92      0.91      0.91      5993\n",
      "weighted avg       0.92      0.91      0.91      5993\n",
      "\n",
      "True Positives (TP): 2896\n",
      "True Negatives (TN): 2586\n",
      "False Positives (FP): 405\n",
      "False Negatives (FN): 106\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## olid + troff model on solid test\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_solidtroff.to(device)\n",
    "\n",
    "model_solidtroff.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_solid:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_solidtroff(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solidtroff.to('cpu')\n",
    "\n",
    "print(\"solid dataset test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_solid, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_solid, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "olid+solid test time:  5.710323095321655  seconds\n",
      "Validation Accuracy: 0.7926165019164817\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.78      0.56       835\n",
      "           1       0.95      0.80      0.86      4122\n",
      "\n",
      "    accuracy                           0.79      4957\n",
      "   macro avg       0.69      0.79      0.71      4957\n",
      "weighted avg       0.86      0.79      0.81      4957\n",
      "\n",
      "True Positives (TP): 3280\n",
      "True Negatives (TN): 649\n",
      "False Positives (FP): 186\n",
      "False Negatives (FN): 842\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## olid + troff model on hso test\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_solidtroff.to(device)\n",
    "\n",
    "model_solidtroff.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_hso:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_solidtroff(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solidtroff.to('cpu')\n",
    "\n",
    "print(\"hso test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_hso, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_hso, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "troff test time:  21.207704067230225  seconds\n",
      "Validation Accuracy: 0.8588904694167852\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.98      0.92      2804\n",
      "           1       0.82      0.38      0.52       711\n",
      "\n",
      "    accuracy                           0.86      3515\n",
      "   macro avg       0.84      0.68      0.72      3515\n",
      "weighted avg       0.85      0.86      0.84      3515\n",
      "\n",
      "True Positives (TP): 273\n",
      "True Negatives (TN): 2746\n",
      "False Positives (FP): 58\n",
      "False Negatives (FN): 438\n"
     ]
    }
   ],
   "source": [
    "## solid + troff model on troff test\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_solidtroff.to(device)\n",
    "\n",
    "model_solidtroff.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_troff:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_solidtroff(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solidtroff.to('cpu')\n",
    "\n",
    "print(\"troff test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_troff, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_troff, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "olid test time:  1.2061831951141357  seconds\n",
      "Validation Accuracy: 0.8395348837209302\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.92      0.89       620\n",
      "           1       0.75      0.64      0.69       240\n",
      "\n",
      "    accuracy                           0.84       860\n",
      "   macro avg       0.81      0.78      0.79       860\n",
      "weighted avg       0.83      0.84      0.84       860\n",
      "\n",
      "True Positives (TP): 153\n",
      "True Negatives (TN): 569\n",
      "False Positives (FP): 51\n",
      "False Negatives (FN): 87\n"
     ]
    }
   ],
   "source": [
    "## solid + troff model on troff test\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_solidtroff.to(device)\n",
    "\n",
    "model_solidtroff.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_olid:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_solidtroff(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solidtroff.to('cpu')\n",
    "\n",
    "print(\"olid test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_olid, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_olid, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "olid test time:  1.206183671951294  seconds\n",
      "Validation Accuracy: 0.8372093023255814\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.92      0.89       620\n",
      "           1       0.75      0.62      0.68       240\n",
      "\n",
      "    accuracy                           0.84       860\n",
      "   macro avg       0.81      0.77      0.79       860\n",
      "weighted avg       0.83      0.84      0.83       860\n",
      "\n",
      "True Positives (TP): 149\n",
      "True Negatives (TN): 571\n",
      "False Positives (FP): 49\n",
      "False Negatives (FN): 91\n"
     ]
    }
   ],
   "source": [
    "## hsomodel on olid test\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_hso.to(device)\n",
    "\n",
    "model_hso.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_olid:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_hso(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solidtroff.to('cpu')\n",
    "\n",
    "print(\"olid test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_olid, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_olid, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "solid test time:  6.338438987731934  seconds\n",
      "Validation Accuracy: 0.9098948773569164\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.87      0.91      2991\n",
      "           1       0.88      0.95      0.91      3002\n",
      "\n",
      "    accuracy                           0.91      5993\n",
      "   macro avg       0.91      0.91      0.91      5993\n",
      "weighted avg       0.91      0.91      0.91      5993\n",
      "\n",
      "True Positives (TP): 2857\n",
      "True Negatives (TN): 2596\n",
      "False Positives (FP): 395\n",
      "False Negatives (FN): 145\n"
     ]
    }
   ],
   "source": [
    "## hsomodel on solid test\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_hso.to(device)\n",
    "\n",
    "model_hso.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_solid:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_hso(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solidtroff.to('cpu')\n",
    "\n",
    "print(\"solid test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_solid, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_solid, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "olid+solid test time:  5.676846027374268  seconds\n",
      "Validation Accuracy: 0.7811176114585435\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.79      0.55       835\n",
      "           1       0.95      0.78      0.86      4122\n",
      "\n",
      "    accuracy                           0.78      4957\n",
      "   macro avg       0.68      0.78      0.70      4957\n",
      "weighted avg       0.86      0.78      0.80      4957\n",
      "\n",
      "True Positives (TP): 3213\n",
      "True Negatives (TN): 659\n",
      "False Positives (FP): 176\n",
      "False Negatives (FN): 909\n"
     ]
    }
   ],
   "source": [
    "## hsomodel on hso test\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_hso.to(device)\n",
    "\n",
    "model_hso.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_hso:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_hso(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solidtroff.to('cpu')\n",
    "\n",
    "print(\"olid+solid test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_hso, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_hso, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
