{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdoga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, ConvBertForSequenceClassification, ConvBertTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load dataset\n",
    "df_olidtest = pd.read_csv('../datasets/cleaned_OLID_test.tsv', sep=\"\\t\")\n",
    "# df = pd.read_csv('datasets/cleaned_OLID.tsv', sep=\"\\t\")\n",
    "df_solid = pd.read_csv('../datasets/cleaned_SOLIDtest6K_trainer.tsv', sep='\\t')\n",
    "df_troff = pd.read_csv('../datasets/cleaned_tr_offenseval_test.tsv', sep='\\t')\n",
    "df_hso = pd.read_csv('../datasets/cleaned_hatespeech_offensive_test.tsv', sep='\\t')\n",
    "\n",
    "# Assuming your columns are named 'tweet' and 'class', change accordingly\n",
    "tweets_olid = df_olidtest['tweet'].values\n",
    "labels_df_olid = df_olidtest['label'].values\n",
    "\n",
    "tweets_solid = df_solid['tweet'].values\n",
    "labels_df_solid = df_solid['label'].values\n",
    "\n",
    "tweets_troff = df_troff['tweet'].values\n",
    "labels_df_troff = df_troff['label'].values\n",
    "\n",
    "tweets_hso = df_hso['tweet'].values\n",
    "labels_df_hso = df_hso['label'].values\n",
    "\n",
    "full_model_name = 'YituTech/conv-bert-base'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(full_model_name)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-emotion\")\n",
    "\n",
    "# Tokenize and encode the training and validation texts\n",
    "# train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True)\n",
    "encodings_olid = tokenizer(tweets_olid.tolist(), truncation=True, padding=True)\n",
    "encodings_solid = tokenizer(tweets_solid.tolist(), truncation=True, padding=True)\n",
    "encodings_troff = tokenizer(tweets_troff.tolist(), truncation=True, padding=True)\n",
    "encodings_hso = tokenizer(tweets_hso.tolist(), truncation=True, padding=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# train_dataset = TweetDataset(train_encodings, train_labels)\n",
    "val_dataset_olid = TweetDataset(encodings_olid, labels_df_olid)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=12, shuffle=True)\n",
    "val_loader_olid = torch.utils.data.DataLoader(val_dataset_olid, batch_size=12, shuffle=False)\n",
    "\n",
    "\n",
    "val_dataset_solid = TweetDataset(encodings_solid, labels_df_solid)\n",
    "val_loader_solid = torch.utils.data.DataLoader(val_dataset_solid, batch_size=12, shuffle=False)\n",
    "\n",
    "val_dataset_troff = TweetDataset(encodings_troff, labels_df_troff)\n",
    "val_loader_troff = torch.utils.data.DataLoader(val_dataset_troff, batch_size=12, shuffle=False)\n",
    "\n",
    "val_dataset_hso = TweetDataset(encodings_hso, labels_df_hso)\n",
    "val_loader_hso = torch.utils.data.DataLoader(val_dataset_hso, batch_size=12, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from transformers import BertForSequenceClassification, ConvBertForSequenceClassification\n",
    "import time\n",
    "model_name = \"conv-bert-base\"\n",
    "\n",
    "model_olid = ConvBertForSequenceClassification.from_pretrained(f\"models/{model_name}_olid\")\n",
    "model_solid = ConvBertForSequenceClassification.from_pretrained(f\"models/{model_name}_solid\")\n",
    "model_olidsolid = ConvBertForSequenceClassification.from_pretrained(f\"models/{model_name}_olid_solid\")\n",
    "model_solidtroff = ConvBertForSequenceClassification.from_pretrained(f\"models/{model_name}_solid_tr\")\n",
    "model_hso = ConvBertForSequenceClassification.from_pretrained(f\"models/{model_name}_hso\")\n",
    "\n",
    "# Define optimizer and learning rate\n",
    "optimizer = optim.AdamW(model_olid.parameters(), lr=1e-5)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "olid dataset test time:  2.590994358062744  seconds\n",
      "Validation Accuracy: 0.8395348837209302\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.88      0.89       620\n",
      "           1       0.70      0.73      0.72       240\n",
      "\n",
      "    accuracy                           0.84       860\n",
      "   macro avg       0.80      0.81      0.80       860\n",
      "weighted avg       0.84      0.84      0.84       860\n",
      "\n",
      "True Positives (TP): 176\n",
      "True Negatives (TN): 546\n",
      "False Positives (FP): 74\n",
      "False Negatives (FN): 64\n"
     ]
    }
   ],
   "source": [
    "## olid model on olid test\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluation\n",
    "model_olid.to(device)\n",
    "model_olid.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_olid:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_olid(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_olid.to('cpu')\n",
    "\n",
    "print(\"olid dataset test time: \", test_end - test_start, \" seconds\")\n",
    "accuracy = correct / total\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_olid, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_olid, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "solid dataset test time:  13.694557428359985  seconds\n",
      "Validation Accuracy: 0.9215751710328717\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.85      0.92      2991\n",
      "           1       0.87      1.00      0.93      3002\n",
      "\n",
      "    accuracy                           0.92      5993\n",
      "   macro avg       0.93      0.92      0.92      5993\n",
      "weighted avg       0.93      0.92      0.92      5993\n",
      "\n",
      "True Positives (TP): 2993\n",
      "True Negatives (TN): 2530\n",
      "False Positives (FP): 461\n",
      "False Negatives (FN): 9\n"
     ]
    }
   ],
   "source": [
    "## olid on solid test\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Evaluation\n",
    "model_olid.to(device)\n",
    "model_olid.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_solid:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_olid(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_olid.to('cpu')\n",
    "\n",
    "print(\"solid dataset test time: \", test_end - test_start, \" seconds\")\n",
    "accuracy = correct / total\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_solid, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_solid, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "hso dataset test time:  12.042570352554321  seconds\n",
      "Validation Accuracy: 0.892071817631632\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.62      0.66       835\n",
      "           1       0.93      0.95      0.94      4122\n",
      "\n",
      "    accuracy                           0.89      4957\n",
      "   macro avg       0.81      0.79      0.80      4957\n",
      "weighted avg       0.89      0.89      0.89      4957\n",
      "\n",
      "True Positives (TP): 3901\n",
      "True Negatives (TN): 521\n",
      "False Positives (FP): 314\n",
      "False Negatives (FN): 221\n"
     ]
    }
   ],
   "source": [
    "## olid model on hso test\n",
    "\n",
    "# Evaluation\n",
    "model_olid.to(device)\n",
    "model_olid.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_hso:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_olid(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_olid.to('cpu')\n",
    "\n",
    "print(\"hso dataset test time: \", test_end - test_start, \" seconds\")\n",
    "accuracy = correct / total\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_hso, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_hso, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "olid dataset test time:  2.401766538619995  seconds\n",
      "Validation Accuracy: 0.8418604651162791\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.92      0.89       620\n",
      "           1       0.75      0.65      0.70       240\n",
      "\n",
      "    accuracy                           0.84       860\n",
      "   macro avg       0.81      0.78      0.79       860\n",
      "weighted avg       0.84      0.84      0.84       860\n",
      "\n",
      "True Positives (TP): 155\n",
      "True Negatives (TN): 569\n",
      "False Positives (FP): 51\n",
      "False Negatives (FN): 85\n"
     ]
    }
   ],
   "source": [
    "## solid model on olid test\n",
    "\n",
    "# Evaluation\n",
    "###\n",
    "model_solid.to(device)\n",
    "model_solid.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_olid: ###\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_solid(input_ids, attention_mask=attention_mask) ###\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solid.to('cpu') ###\n",
    "\n",
    "print(\"olid dataset test time: \", test_end - test_start, \" seconds\")\n",
    "accuracy = correct / total\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_olid, prediction_list)) ###\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_olid, prediction_list) ###\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "solid dataset test time:  13.657119512557983  seconds\n",
      "Validation Accuracy: 0.9170699149007175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.86      0.91      2991\n",
      "           1       0.87      0.98      0.92      3002\n",
      "\n",
      "    accuracy                           0.92      5993\n",
      "   macro avg       0.92      0.92      0.92      5993\n",
      "weighted avg       0.92      0.92      0.92      5993\n",
      "\n",
      "True Positives (TP): 2929\n",
      "True Negatives (TN): 2567\n",
      "False Positives (FP): 424\n",
      "False Negatives (FN): 73\n"
     ]
    }
   ],
   "source": [
    "## solid model on solid test\n",
    "\n",
    "# Evaluation\n",
    "###\n",
    "model_solid.to(device)\n",
    "model_solid.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_solid: ###\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_solid(input_ids, attention_mask=attention_mask) ###\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solid.to('cpu') ###\n",
    "\n",
    "print(\"solid dataset test time: \", test_end - test_start, \" seconds\")\n",
    "accuracy = correct / total\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_solid, prediction_list)) ###\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_solid, prediction_list) ###\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "hso dataset test time:  12.246433019638062  seconds\n",
      "Validation Accuracy: 0.8176316320355054\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.78      0.59       835\n",
      "           1       0.95      0.83      0.88      4122\n",
      "\n",
      "    accuracy                           0.82      4957\n",
      "   macro avg       0.71      0.80      0.74      4957\n",
      "weighted avg       0.87      0.82      0.83      4957\n",
      "\n",
      "True Positives (TP): 3401\n",
      "True Negatives (TN): 652\n",
      "False Positives (FP): 183\n",
      "False Negatives (FN): 721\n"
     ]
    }
   ],
   "source": [
    "## solid model on hso test\n",
    "\n",
    "# Evaluation\n",
    "###\n",
    "model_solid.to(device)\n",
    "model_solid.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_hso: ###\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_solid(input_ids, attention_mask=attention_mask) ###\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solid.to('cpu') ###\n",
    "\n",
    "print(\"hso dataset test time: \", test_end - test_start, \" seconds\")\n",
    "accuracy = correct / total\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_hso, prediction_list)) ###\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_hso, prediction_list) ###\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "olid dataset test time:  2.4198782444000244  seconds\n",
      "Validation Accuracy: 0.8104651162790698\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.82      0.86       620\n",
      "           1       0.63      0.78      0.70       240\n",
      "\n",
      "    accuracy                           0.81       860\n",
      "   macro avg       0.77      0.80      0.78       860\n",
      "weighted avg       0.83      0.81      0.82       860\n",
      "\n",
      "True Positives (TP): 187\n",
      "True Negatives (TN): 510\n",
      "False Positives (FP): 110\n",
      "False Negatives (FN): 53\n"
     ]
    }
   ],
   "source": [
    "## olid + solid model on olid test\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_olidsolid.to(device)\n",
    "\n",
    "model_olidsolid.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_olid:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_olidsolid(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_olidsolid.to('cpu')\n",
    "\n",
    "print(\"olid dataset test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_olid, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_olid, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "solid dataset test time:  13.695028305053711  seconds\n",
      "Validation Accuracy: 0.9170699149007175\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.84      0.91      2991\n",
      "           1       0.86      0.99      0.92      3002\n",
      "\n",
      "    accuracy                           0.92      5993\n",
      "   macro avg       0.93      0.92      0.92      5993\n",
      "weighted avg       0.93      0.92      0.92      5993\n",
      "\n",
      "True Positives (TP): 2983\n",
      "True Negatives (TN): 2513\n",
      "False Positives (FP): 478\n",
      "False Negatives (FN): 19\n"
     ]
    }
   ],
   "source": [
    "## olid + solid model on solid test\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_olidsolid.to(device)\n",
    "\n",
    "model_olidsolid.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_solid:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_olidsolid(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_olidsolid.to('cpu')\n",
    "\n",
    "print(\"solid dataset test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_solid, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_solid, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "hso dataset test time:  12.262091875076294  seconds\n",
      "Validation Accuracy: 0.8301391970950172\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.69      0.58       835\n",
      "           1       0.93      0.86      0.89      4122\n",
      "\n",
      "    accuracy                           0.83      4957\n",
      "   macro avg       0.71      0.78      0.74      4957\n",
      "weighted avg       0.86      0.83      0.84      4957\n",
      "\n",
      "True Positives (TP): 3537\n",
      "True Negatives (TN): 578\n",
      "False Positives (FP): 257\n",
      "False Negatives (FN): 585\n"
     ]
    }
   ],
   "source": [
    "## olid + solid model on hso test\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_olidsolid.to(device)\n",
    "\n",
    "model_olidsolid.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_hso:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_olidsolid(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_olidsolid.to('cpu')\n",
    "\n",
    "print(\"hso dataset test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_hso, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_hso, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "olid datset test time:  2.473078489303589  seconds\n",
      "Validation Accuracy: 0.8406976744186047\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.97      0.90       620\n",
      "           1       0.86      0.51      0.64       240\n",
      "\n",
      "    accuracy                           0.84       860\n",
      "   macro avg       0.85      0.74      0.77       860\n",
      "weighted avg       0.84      0.84      0.83       860\n",
      "\n",
      "True Positives (TP): 123\n",
      "True Negatives (TN): 600\n",
      "False Positives (FP): 20\n",
      "False Negatives (FN): 117\n"
     ]
    }
   ],
   "source": [
    "## solid + troff model on olid test\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_solidtroff.to(device)\n",
    "\n",
    "model_solidtroff.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_olid:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_solidtroff(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solidtroff.to('cpu')\n",
    "\n",
    "print(\"olid datset test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_olid, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_olid, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "solid dataset test time:  13.583001613616943  seconds\n",
      "Validation Accuracy: 0.9123977974303354\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.90      0.91      2991\n",
      "           1       0.90      0.93      0.91      3002\n",
      "\n",
      "    accuracy                           0.91      5993\n",
      "   macro avg       0.91      0.91      0.91      5993\n",
      "weighted avg       0.91      0.91      0.91      5993\n",
      "\n",
      "True Positives (TP): 2780\n",
      "True Negatives (TN): 2688\n",
      "False Positives (FP): 303\n",
      "False Negatives (FN): 222\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## olid + troff model on solid test\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_solidtroff.to(device)\n",
    "\n",
    "model_solidtroff.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_solid:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_solidtroff(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solidtroff.to('cpu')\n",
    "\n",
    "print(\"solid dataset test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_solid, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_solid, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "hso test time:  12.239339590072632  seconds\n",
      "Validation Accuracy: 0.7924147669961671\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.91      0.60       835\n",
      "           1       0.98      0.77      0.86      4122\n",
      "\n",
      "    accuracy                           0.79      4957\n",
      "   macro avg       0.71      0.84      0.73      4957\n",
      "weighted avg       0.89      0.79      0.82      4957\n",
      "\n",
      "True Positives (TP): 3170\n",
      "True Negatives (TN): 758\n",
      "False Positives (FP): 77\n",
      "False Negatives (FN): 952\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## olid + troff model on hso test\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_solidtroff.to(device)\n",
    "\n",
    "model_solidtroff.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_hso:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_solidtroff(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solidtroff.to('cpu')\n",
    "\n",
    "print(\"hso test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_hso, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_hso, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "troff test time:  40.16333270072937  seconds\n",
      "Validation Accuracy: 0.8463726884779517\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.99      0.91      2804\n",
      "           1       0.89      0.28      0.42       711\n",
      "\n",
      "    accuracy                           0.85      3515\n",
      "   macro avg       0.87      0.63      0.67      3515\n",
      "weighted avg       0.85      0.85      0.81      3515\n",
      "\n",
      "True Positives (TP): 196\n",
      "True Negatives (TN): 2779\n",
      "False Positives (FP): 25\n",
      "False Negatives (FN): 515\n"
     ]
    }
   ],
   "source": [
    "## solid + troff model on troff test\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_solidtroff.to(device)\n",
    "\n",
    "model_solidtroff.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_troff:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_solidtroff(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solidtroff.to('cpu')\n",
    "\n",
    "print(\"troff test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_troff, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_troff, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "olid test time:  2.4205334186553955  seconds\n",
      "Validation Accuracy: 0.8430232558139535\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.94      0.90       620\n",
      "           1       0.80      0.59      0.68       240\n",
      "\n",
      "    accuracy                           0.84       860\n",
      "   macro avg       0.83      0.76      0.79       860\n",
      "weighted avg       0.84      0.84      0.83       860\n",
      "\n",
      "True Positives (TP): 141\n",
      "True Negatives (TN): 584\n",
      "False Positives (FP): 36\n",
      "False Negatives (FN): 99\n"
     ]
    }
   ],
   "source": [
    "## hsomodel on olid test\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_hso.to(device)\n",
    "\n",
    "model_hso.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_olid:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_hso(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solidtroff.to('cpu')\n",
    "\n",
    "print(\"olid test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_olid, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_olid, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "solid test time:  13.575922012329102  seconds\n",
      "Validation Accuracy: 0.913565826797931\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.88      0.91      2991\n",
      "           1       0.89      0.95      0.92      3002\n",
      "\n",
      "    accuracy                           0.91      5993\n",
      "   macro avg       0.92      0.91      0.91      5993\n",
      "weighted avg       0.92      0.91      0.91      5993\n",
      "\n",
      "True Positives (TP): 2838\n",
      "True Negatives (TN): 2637\n",
      "False Positives (FP): 354\n",
      "False Negatives (FN): 164\n"
     ]
    }
   ],
   "source": [
    "## hsomodel on solid test\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_hso.to(device)\n",
    "\n",
    "model_hso.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_solid:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_hso(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solidtroff.to('cpu')\n",
    "\n",
    "print(\"solid test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_solid, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_solid, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "end\n",
      "olid+solid test time:  12.279685974121094  seconds\n",
      "Validation Accuracy: 0.7964494654024612\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.45      0.86      0.59       835\n",
      "           1       0.96      0.78      0.86      4122\n",
      "\n",
      "    accuracy                           0.80      4957\n",
      "   macro avg       0.71      0.82      0.73      4957\n",
      "weighted avg       0.88      0.80      0.82      4957\n",
      "\n",
      "True Positives (TP): 3232\n",
      "True Negatives (TN): 716\n",
      "False Positives (FP): 119\n",
      "False Negatives (FN): 890\n"
     ]
    }
   ],
   "source": [
    "## hsomodel on hso test\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "model_hso.to(device)\n",
    "\n",
    "model_hso.eval()\n",
    "\n",
    "# Perform evaluation on validation set and calculate metrics as needed\n",
    "# Example: calculate accuracy\n",
    "correct = 0\n",
    "total = 0\n",
    "i = 0\n",
    "prediction_list = np.array([])\n",
    "with torch.no_grad():\n",
    "    test_start  = time.time()\n",
    "    print('start')\n",
    "    for batch in val_loader_hso:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model_hso(input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs.logits, dim=1)\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "    print('end')\n",
    "    \n",
    "    test_end = time.time()\n",
    "\n",
    "model_solidtroff.to('cpu')\n",
    "\n",
    "print(\"olid+solid test time: \", test_end - test_start, \" seconds\")\n",
    "\n",
    "accuracy = correct / total\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy}')\n",
    "print(classification_report(labels_df_hso, prediction_list))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "cm = confusion_matrix(labels_df_hso, prediction_list)\n",
    "\n",
    "# Extract TP, TN, FP, FN from the confusion matrix\n",
    "TP = cm[1, 1]\n",
    "TN = cm[0, 0]\n",
    "FP = cm[0, 1]\n",
    "FN = cm[1, 0]\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
