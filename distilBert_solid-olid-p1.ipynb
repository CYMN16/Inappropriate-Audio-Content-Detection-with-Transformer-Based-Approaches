{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.020356416702270508\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "timers = []\n",
    "timers_import_start = time.time() \n",
    "# Trainer (has labels)\n",
    "#trainer_df = pd.read_csv('../datasets/cleaned_SOLIDtest6K_trainer.tsv', sep=\"\\t\")\n",
    "learner_tweets_df = pd.read_csv('datasets/cleaned_OLID.tsv', sep=\"\\t\")\n",
    "timers_import_end = time.time()\n",
    "print(timers_import_end - timers_import_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# trainer_labels = trainer_df['label'].values\n",
    "# trainer_tweets = trainer_df['tweet'].values\n",
    "#learner_tweets_df['labels'] = learner_tweets_df['average'].apply(lambda x: 1 if x >= 0.8 else 0) # threshold the average values\n",
    "# learner_tweets_df['subtask_a'] = learner_tweets_df['subtask_a'].map({'OFF': 1, 'NOT': 0})\n",
    "\n",
    "# learner_tweets_df['labels'] = learner_tweets_df[\"subtask_a\"]\n",
    "# learner_tweets_df.drop([\"subtask_a\",\"subtask_b\",\"subtask_c\"], axis=1, inplace=True)\n",
    "sample_size = 40000\n",
    "positive_ratio = 0.5\n",
    "\n",
    "# Select the most confident positive values\n",
    "#semi_tweets_pos_df = learner_tweets_df[learner_tweets_df['average'] > 0.8].sample(n=np.floor(sample_size*positive_ratio).astype(int), random_state=1)\n",
    "\n",
    "# Select the most confident negative values\n",
    "#semi_tweets_neg_df = learner_tweets_df[learner_tweets_df['average'] < 0.3].sample(n=np.floor(sample_size*(1-positive_ratio)).astype(int), random_state=1)\n",
    "\n",
    "#semi_tweets_df = pd.concat([semi_tweets_pos_df, semi_tweets_neg_df])\n",
    "semi_tweets_df = learner_tweets_df\n",
    "semi_tweets_df = semi_tweets_df.sample(frac=1, random_state=42)\n",
    "\n",
    "semi_tweets = semi_tweets_df['tweet'].values\n",
    "semi_labels = semi_tweets_df['label'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12387</th>\n",
       "      <td>27650</td>\n",
       "      <td>why is john kerry running his mouth again as i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>52965</td>\n",
       "      <td>gun control anyone disarmhate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>87438</td>\n",
       "      <td>what manga you reading</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3971</th>\n",
       "      <td>67140</td>\n",
       "      <td>you are right victoria is on the reverse</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10394</th>\n",
       "      <td>43102</td>\n",
       "      <td>thank you   america is respected again maga ka...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12889</th>\n",
       "      <td>25562</td>\n",
       "      <td>they are antifa democrats disguised as gop loo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4504</th>\n",
       "      <td>27304</td>\n",
       "      <td>how do you come up with all these lies you hav...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3268</th>\n",
       "      <td>56638</td>\n",
       "      <td>bitch my life was flashing before my eyes</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>49314</td>\n",
       "      <td>lmao what a joke he is</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4688</th>\n",
       "      <td>42416</td>\n",
       "      <td>looking forward to the right dragging up these...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              tweet  label\n",
       "12387  27650  why is john kerry running his mouth again as i...      0\n",
       "385    52965                      gun control anyone disarmhate      0\n",
       "4896   87438                             what manga you reading      0\n",
       "3971   67140           you are right victoria is on the reverse      0\n",
       "10394  43102  thank you   america is respected again maga ka...      0\n",
       "12889  25562  they are antifa democrats disguised as gop loo...      0\n",
       "4504   27304  how do you come up with all these lies you hav...      1\n",
       "3268   56638          bitch my life was flashing before my eyes      1\n",
       "2439   49314                             lmao what a joke he is      0\n",
       "4688   42416  looking forward to the right dragging up these...      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semi_tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            return {\n",
    "                'text': text,\n",
    "                'label': label\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'text': text\n",
    "            }\n",
    "        \n",
    "# trainer_dataset = TweetDataset(trainer_tweets, trainer_labels)\n",
    "learner_dataset = TweetDataset(semi_tweets, semi_labels)\n",
    "\n",
    "# trainer_loader = DataLoader(trainer_dataset, batch_size=12, shuffle=True)\n",
    "learner_loader = DataLoader(learner_dataset, batch_size=6, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cymn/miniconda3/envs/nvidia-newest/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "from transformers import DistilBertTokenizer, DistilBertModel, DistilBertForSequenceClassification\n",
    "\n",
    "# Number of epochs\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert/distilbert-base-uncased')\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\", num_labels=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Set up the optimizer and loss function\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2207 [00:00<?, ?it/s]/tmp/ipykernel_47463/692897503.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(batch['label']).clone().detach().to(device)\n",
      "100%|██████████| 2207/2207 [01:15<00:00, 29.31it/s]\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    # trainer_dataloader_iterator = iter(trainer_loader)\n",
    "    semi_labeled_dataloader_iterator = iter(learner_loader)\n",
    "    num_batches = len(learner_loader)\n",
    "    # num_batches = 5000\n",
    "    for _ in tqdm(range(num_batches)):\n",
    "        # Train on labeled data\n",
    "        batch = next(semi_labeled_dataloader_iterator, None)\n",
    "        if batch is not None:\n",
    "            inputs = tokenizer(batch['text'], padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "            labels = torch.tensor(batch['label']).clone().detach().to(device)\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "    \n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('models/SOLIDOLID_80k_DISTILBERT_1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
