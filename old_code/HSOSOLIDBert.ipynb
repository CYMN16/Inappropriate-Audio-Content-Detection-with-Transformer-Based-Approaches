{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.722425937652588\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "timers = []\n",
    "timers_import_start = time.time() \n",
    "# Trainer (has labels)\n",
    "#trainer_df = pd.read_csv('../datasets/cleaned_SOLIDtest6K_trainer.tsv', sep=\"\\t\")\n",
    "# learner_tweets_df = pd.read_csv('datasets/cleaned_OLID.tsv', sep=\"\\t\")\n",
    "# learner_tweets_df = pd.read_csv('datasets/cleaned_hatespeech_offensive.tsv', sep=\"\\t\")\n",
    "learner_tweets_df = pd.read_csv('datasets/cleaned_SOLID9M_learner.tsv', sep=\"\\t\")\n",
    "\n",
    "timers_import_end = time.time()\n",
    "print(timers_import_end - timers_import_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# trainer_labels = trainer_df['label'].values\n",
    "# trainer_tweets = trainer_df['tweet'].values\n",
    "learner_tweets_df['labels'] = learner_tweets_df['average'].apply(lambda x: 1 if x >= 0.8 else 0) # threshold the average values\n",
    "# learner_tweets_df['subtask_a'] = learner_tweets_df['subtask_a'].map({'OFF': 1, 'NOT': 0})\n",
    "\n",
    "# learner_tweets_df['labels'] = learner_tweets_df[\"subtask_a\"]\n",
    "# learner_tweets_df.drop([\"subtask_a\",\"subtask_b\",\"subtask_c\"], axis=1, inplace=True)\n",
    "sample_size = 40000\n",
    "positive_ratio = 0.5\n",
    "\n",
    "# Select the most confident positive values\n",
    "semi_tweets_pos_df = learner_tweets_df[learner_tweets_df['average'] > 0.8].sample(n=np.floor(sample_size*positive_ratio).astype(int), random_state=1)\n",
    "\n",
    "# Select the most confident negative values\n",
    "semi_tweets_neg_df = learner_tweets_df[learner_tweets_df['average'] < 0.3].sample(n=np.floor(sample_size*(1-positive_ratio)).astype(int), random_state=1)\n",
    "\n",
    "semi_tweets_df = pd.concat([semi_tweets_pos_df, semi_tweets_neg_df])\n",
    "# semi_tweets_df = learner_tweets_df\n",
    "\n",
    "semi_tweets_df = semi_tweets_df.sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "# semi_tweets = semi_tweets_df['tweet'].values\n",
    "semi_tweets = semi_tweets_df['text'].values\n",
    "# semi_labels = semi_tweets_df['label'].values\n",
    "semi_labels = semi_tweets_df['labels'].values\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(semi_tweets, semi_labels, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>average</th>\n",
       "      <th>std</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2317492</th>\n",
       "      <td>1160056414993207296</td>\n",
       "      <td>i am so blessed to have a hard working man tha...</td>\n",
       "      <td>0.183495</td>\n",
       "      <td>0.188825</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3493021</th>\n",
       "      <td>1162174496058462210</td>\n",
       "      <td>we are all slowly eroding toward our inevitabl...</td>\n",
       "      <td>0.809006</td>\n",
       "      <td>0.167128</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6721063</th>\n",
       "      <td>1187719901819133953</td>\n",
       "      <td>well he had the guts to stay in lebanon throug...</td>\n",
       "      <td>0.290353</td>\n",
       "      <td>0.206593</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4158032</th>\n",
       "      <td>1156012291923435527</td>\n",
       "      <td>should i put neons under my car old school fas...</td>\n",
       "      <td>0.844974</td>\n",
       "      <td>0.145038</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1989247</th>\n",
       "      <td>1160353266678599681</td>\n",
       "      <td>i am desperate for some company bc i am about ...</td>\n",
       "      <td>0.287112</td>\n",
       "      <td>0.125938</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87484</th>\n",
       "      <td>1159669733601566720</td>\n",
       "      <td>you are so sweet tysm</td>\n",
       "      <td>0.163308</td>\n",
       "      <td>0.192813</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8303403</th>\n",
       "      <td>1187343402914013186</td>\n",
       "      <td>fuck all that talking i would of shot him already</td>\n",
       "      <td>0.822159</td>\n",
       "      <td>0.162010</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7440959</th>\n",
       "      <td>1188518751622221824</td>\n",
       "      <td>i hear colorado is a great place for raising a...</td>\n",
       "      <td>0.190064</td>\n",
       "      <td>0.186808</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4699260</th>\n",
       "      <td>1156407147753816064</td>\n",
       "      <td>do not act like ur bf could kick my ass i dead...</td>\n",
       "      <td>0.804773</td>\n",
       "      <td>0.146065</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2610366</th>\n",
       "      <td>1160213285754605568</td>\n",
       "      <td>hearts going to burst the coupon</td>\n",
       "      <td>0.297978</td>\n",
       "      <td>0.165141</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id  \\\n",
       "2317492  1160056414993207296   \n",
       "3493021  1162174496058462210   \n",
       "6721063  1187719901819133953   \n",
       "4158032  1156012291923435527   \n",
       "1989247  1160353266678599681   \n",
       "87484    1159669733601566720   \n",
       "8303403  1187343402914013186   \n",
       "7440959  1188518751622221824   \n",
       "4699260  1156407147753816064   \n",
       "2610366  1160213285754605568   \n",
       "\n",
       "                                                      text   average  \\\n",
       "2317492  i am so blessed to have a hard working man tha...  0.183495   \n",
       "3493021  we are all slowly eroding toward our inevitabl...  0.809006   \n",
       "6721063  well he had the guts to stay in lebanon throug...  0.290353   \n",
       "4158032  should i put neons under my car old school fas...  0.844974   \n",
       "1989247  i am desperate for some company bc i am about ...  0.287112   \n",
       "87484                                you are so sweet tysm  0.163308   \n",
       "8303403  fuck all that talking i would of shot him already  0.822159   \n",
       "7440959  i hear colorado is a great place for raising a...  0.190064   \n",
       "4699260  do not act like ur bf could kick my ass i dead...  0.804773   \n",
       "2610366                   hearts going to burst the coupon  0.297978   \n",
       "\n",
       "              std  labels  \n",
       "2317492  0.188825       0  \n",
       "3493021  0.167128       1  \n",
       "6721063  0.206593       0  \n",
       "4158032  0.145038       1  \n",
       "1989247  0.125938       0  \n",
       "87484    0.192813       0  \n",
       "8303403  0.162010       1  \n",
       "7440959  0.186808       0  \n",
       "4699260  0.146065       1  \n",
       "2610366  0.165141       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semi_tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            return {\n",
    "                'text': text,\n",
    "                'label': label\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'text': text\n",
    "            }\n",
    "        \n",
    "learner_dataset = TweetDataset(semi_tweets, semi_labels)\n",
    "# learner_dataset = TweetDataset(X_train, y_train)\n",
    "# test_dataset = TweetDataset(X_test, y_test)\n",
    "\n",
    "# trainer_loader = DataLoader(trainer_dataset, batch_size=12, shuffle=True)\n",
    "learner_loader = DataLoader(learner_dataset, batch_size=6, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=6, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cymn/miniconda3/envs/nvidia-newest/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Number of epochs\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "\n",
    "# Set up the optimizer and loss function\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6667 [00:00<?, ?it/s]/tmp/ipykernel_53193/692897503.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(batch['label']).clone().detach().to(device)\n",
      "100%|██████████| 6667/6667 [05:21<00:00, 20.72it/s]\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    # trainer_dataloader_iterator = iter(trainer_loader)\n",
    "    semi_labeled_dataloader_iterator = iter(learner_loader)\n",
    "    num_batches = len(learner_loader)\n",
    "    # num_batches = 5000\n",
    "    for _ in tqdm(range(num_batches)):\n",
    "        # Train on labeled data\n",
    "        batch = next(semi_labeled_dataloader_iterator, None)\n",
    "        if batch is not None:\n",
    "            inputs = tokenizer(batch['text'], padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "            labels = torch.tensor(batch['label']).clone().detach().to(device)\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "    \n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# prediction_list = np.array([])\n",
    "# model.eval()\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# num_epochs = 1\n",
    "# for epoch in range(num_epochs):\n",
    "#     # trainer_dataloader_iterator = iter(trainer_loader)\n",
    "#     semi_labeled_dataloader_iterator = iter(test_loader)\n",
    "#     num_batches = len(test_loader)\n",
    "#     # num_batches = 5000\n",
    "#     for _ in tqdm(range(num_batches)):\n",
    "#         # Train on labeled data\n",
    "#         batch = next(semi_labeled_dataloader_iterator, None)\n",
    "#         if batch is not None:\n",
    "#             inputs = tokenizer(batch['text'], padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "#             labels = torch.tensor(batch['label']).clone().detach().to(device)\n",
    "#             outputs = model(**inputs)\n",
    "#             logits = outputs.logits\n",
    "#             predictions = torch.argmax(outputs.logits, dim=1)\n",
    "#             correct += (predictions == labels).sum().item()\n",
    "#             loss = criterion(logits, labels)\n",
    "#             total += labels.size(0)\n",
    "#             prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "# accuracy = correct / total\n",
    "# print(f'Validation Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# # Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "# report = classification_report(y_test, prediction_list)\n",
    "\n",
    "# print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# # Assuming you have the true labels in `val_labels` and the predicted labels in `prediction_list`\n",
    "# cm = confusion_matrix(y_test, prediction_list)\n",
    "\n",
    "# # Extract TP, TN, FP, FN from the confusion matrix\n",
    "# TP = cm[1, 1]\n",
    "# TN = cm[0, 0]\n",
    "# FP = cm[0, 1]\n",
    "# FN = cm[1, 0]\n",
    "\n",
    "# print(f\"True Positives (TP): {TP}\")\n",
    "# print(f\"True Negatives (TN): {TN}\")\n",
    "# print(f\"False Positives (FP): {FP}\")\n",
    "# print(f\"False Negatives (FN): {FN}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('models/HSOSOLID_BERT_1')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
