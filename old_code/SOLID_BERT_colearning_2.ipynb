{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Trainer (has labels)\n",
    "trainer_df = pd.read_csv('datasets/cleaned_SOLIDtest6K_trainer.tsv', sep=\"\\t\")\n",
    "learner_tweets_df = pd.read_csv('datasets/cleaned_SOLID9M_learner.tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# trainer_labels = trainer_df['label'].values\n",
    "# trainer_tweets = trainer_df['tweet'].values\n",
    "learner_tweets_df['labels'] = learner_tweets_df['average'].apply(lambda x: 1 if x >= 0.5 else 0) # threshold the average values\n",
    "\n",
    "sample_size = 80000\n",
    "positive_ratio = 0.50\n",
    "\n",
    "# Select the most confident positive values\n",
    "semi_tweets_pos_df = learner_tweets_df[learner_tweets_df['average'] > 0.5].sample(n=np.floor(sample_size*positive_ratio).astype(int), random_state=1)\n",
    "\n",
    "# Select the most confident negative values\n",
    "semi_tweets_neg_df = learner_tweets_df[learner_tweets_df['average'] < 0.2].sample(n=np.floor(sample_size*(1-positive_ratio)).astype(int), random_state=1)\n",
    "\n",
    "semi_tweets_df = pd.concat([semi_tweets_pos_df, semi_tweets_neg_df])\n",
    "semi_tweets_df = semi_tweets_df.sample(frac=1, random_state=42)\n",
    "\n",
    "semi_tweets = semi_tweets_df['text'].values\n",
    "semi_labels = semi_tweets_df['labels'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>average</th>\n",
       "      <th>std</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6411147</th>\n",
       "      <td>1187178672442220544</td>\n",
       "      <td>had fish tacos the other day</td>\n",
       "      <td>0.195709</td>\n",
       "      <td>0.188270</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2134644</th>\n",
       "      <td>1161313658473000960</td>\n",
       "      <td>think they are saying it was too windy</td>\n",
       "      <td>0.183525</td>\n",
       "      <td>0.194722</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2772179</th>\n",
       "      <td>1161016801351700480</td>\n",
       "      <td>is the cover of the uk ed from a movie  it loo...</td>\n",
       "      <td>0.178485</td>\n",
       "      <td>0.175064</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3696591</th>\n",
       "      <td>1161756492032348160</td>\n",
       "      <td>he is had a whole pre season though</td>\n",
       "      <td>0.198720</td>\n",
       "      <td>0.201089</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5586179</th>\n",
       "      <td>1155735694284689408</td>\n",
       "      <td>take me to the top take me take me higher</td>\n",
       "      <td>0.179318</td>\n",
       "      <td>0.194545</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3605572</th>\n",
       "      <td>1161423107808256001</td>\n",
       "      <td>as long as this guy keeps running his mouth an...</td>\n",
       "      <td>0.561974</td>\n",
       "      <td>0.139245</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2799246</th>\n",
       "      <td>1161552623566110720</td>\n",
       "      <td>i want to have a date with</td>\n",
       "      <td>0.194520</td>\n",
       "      <td>0.185750</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2725602</th>\n",
       "      <td>1160937939804643328</td>\n",
       "      <td>the feeling nya btob</td>\n",
       "      <td>0.175092</td>\n",
       "      <td>0.180342</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611207</th>\n",
       "      <td>1187283571687473152</td>\n",
       "      <td>i could eat a horse hooves and all</td>\n",
       "      <td>0.511584</td>\n",
       "      <td>0.116744</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4207592</th>\n",
       "      <td>1156095150449463297</td>\n",
       "      <td>hey guys   i have not been on this account in ...</td>\n",
       "      <td>0.196998</td>\n",
       "      <td>0.176672</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id  \\\n",
       "6411147  1187178672442220544   \n",
       "2134644  1161313658473000960   \n",
       "2772179  1161016801351700480   \n",
       "3696591  1161756492032348160   \n",
       "5586179  1155735694284689408   \n",
       "3605572  1161423107808256001   \n",
       "2799246  1161552623566110720   \n",
       "2725602  1160937939804643328   \n",
       "7611207  1187283571687473152   \n",
       "4207592  1156095150449463297   \n",
       "\n",
       "                                                      text   average  \\\n",
       "6411147                       had fish tacos the other day  0.195709   \n",
       "2134644             think they are saying it was too windy  0.183525   \n",
       "2772179  is the cover of the uk ed from a movie  it loo...  0.178485   \n",
       "3696591                he is had a whole pre season though  0.198720   \n",
       "5586179          take me to the top take me take me higher  0.179318   \n",
       "3605572  as long as this guy keeps running his mouth an...  0.561974   \n",
       "2799246                         i want to have a date with  0.194520   \n",
       "2725602                               the feeling nya btob  0.175092   \n",
       "7611207                 i could eat a horse hooves and all  0.511584   \n",
       "4207592  hey guys   i have not been on this account in ...  0.196998   \n",
       "\n",
       "              std  labels  \n",
       "6411147  0.188270       0  \n",
       "2134644  0.194722       0  \n",
       "2772179  0.175064       0  \n",
       "3696591  0.201089       0  \n",
       "5586179  0.194545       0  \n",
       "3605572  0.139245       1  \n",
       "2799246  0.185750       0  \n",
       "2725602  0.180342       0  \n",
       "7611207  0.116744       1  \n",
       "4207592  0.176672       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semi_tweets_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, texts, labels=None):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            return {\n",
    "                'text': text,\n",
    "                'label': label\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'text': text\n",
    "            }\n",
    "        \n",
    "# trainer_dataset = TweetDataset(trainer_tweets, trainer_labels)\n",
    "learner_dataset = TweetDataset(semi_tweets, semi_labels)\n",
    "\n",
    "# trainer_loader = DataLoader(trainer_dataset, batch_size=12, shuffle=True)\n",
    "learner_loader = DataLoader(learner_dataset, batch_size=6, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cymn/miniconda3/envs/nvidia-newest/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('models/OLID_BERT_1', num_labels=2)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13334 [00:00<?, ?it/s]/tmp/ipykernel_5359/1995336368.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(batch['label']).clone().detach().to(device)\n",
      "100%|██████████| 13334/13334 [11:24<00:00, 19.47it/s]\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    # trainer_dataloader_iterator = iter(trainer_loader)\n",
    "    semi_labeled_dataloader_iterator = iter(learner_loader)\n",
    "    num_batches = len(learner_loader)\n",
    "    # num_batches = 5000\n",
    "    for _ in tqdm(range(num_batches)):\n",
    "        # Train on labeled data\n",
    "        batch = next(semi_labeled_dataloader_iterator, None)\n",
    "        if batch is not None:\n",
    "            inputs = tokenizer(batch['text'], padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "            labels = torch.tensor(batch['label']).clone().detach().to(device)\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('models/SOLID_BERT_colearning_2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvidia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
