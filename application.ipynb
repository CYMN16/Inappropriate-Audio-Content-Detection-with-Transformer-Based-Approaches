{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class TweetDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whisper_pipeline(audio_path, json_path):\n",
    "    import whisper_timestamped as whisper_t\n",
    "\n",
    "    audio = whisper_t.load_audio(audio_path)\n",
    "\n",
    "    model = whisper_t.load_model(\"medium\")\n",
    "\n",
    "    # print(model)\n",
    "    result = whisper_t.transcribe(model, audio, language='en')\n",
    "    with open(json_path,'w') as f:\n",
    "        json.dump(result,f)\n",
    "\n",
    "    model.to(torch.device('cpu'))\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SOLID_pipeline(json_in_path, json_out_path):\n",
    "    import torch\n",
    "    from transformers import BertForSequenceClassification, BertTokenizer\n",
    "    import pandas as pd\n",
    "    model = BertForSequenceClassification.from_pretrained('models/SOLID_finetuneHSO_1')\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    data = ''\n",
    "    with open(json_in_path, 'r') as f:\n",
    "        data = json.loads(f.read())\n",
    "        f.close()\n",
    "\n",
    "    data = data['segments']\n",
    "    df = pd.DataFrame(columns=[\"start\",\"end\",\"text\"])\n",
    "    for d in data:\n",
    "        start = d.get(\"start\")\n",
    "        end = d.get(\"end\")\n",
    "        text = d.get(\"text\")\n",
    "        df.loc[len(df)] = [start,end,text]\n",
    "\n",
    "    text = df['text'].values\n",
    "    \n",
    "    encodings = tokenizer(text.tolist(), truncation=True, padding=True)\n",
    "\n",
    "    dataset = TweetDataset(encodings, np.zeros(text.shape))\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    print(model)\n",
    "    \n",
    "    model.eval()\n",
    "    # Perform evaluation on validation set and calculate metrics as needed\n",
    "    # Example: calculate accuracy\n",
    "    # correct = 0\n",
    "    # total = 0\n",
    "    i = 0\n",
    "    prediction_list = np.array([])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # test_start  = time.time()\n",
    "        print('start')\n",
    "        for batch in loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            predictions = torch.argmax(outputs.logits, dim=1)\n",
    "            prediction_list = np.append(prediction_list, predictions.detach().cpu().numpy())\n",
    "        print('end')\n",
    "        \n",
    "        # test_end = time.time()\n",
    "    \n",
    "    df_out = df.assign(predictions=prediction_list)\n",
    "\n",
    "    df_out.to_json(json_out_path)\n",
    "\n",
    "    model.to(torch.device('cpu'))\n",
    "    del model\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_audio_model(audio_path):\n",
    "    data = {'default':'true', 'audio':'false'}\n",
    "    temp_file_path = './json_samples/obj_sample.json'\n",
    "\n",
    "    if audio_path != None:\n",
    "        whisper_pipeline(audio_path, temp_file_path)\n",
    "        with open(temp_file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            f.close()\n",
    "    else:\n",
    "        data = {'default':'false', 'audio':'false'}\n",
    "    out_f = open(temp_file_path, 'w')\n",
    "    json.dump(data, fp=out_f)\n",
    "    out_f.close()\n",
    "    return [data, gr.File(value=temp_file_path, file_types=['.json'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_text_model(input_filepath):\n",
    "    output_filepath = 'json_samples/output_sample.json'\n",
    "    if type(input_filepath) == dict:\n",
    "        return input_filepath\n",
    "    elif type(input_filepath) == gr.utils.NamedString:\n",
    "        SOLID_pipeline(input_filepath, output_filepath)\n",
    "        return json.load(open(output_filepath)) #'This is the text inference output'\n",
    "    else:\n",
    "        return None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with gr.Blocks() as interface:\n",
    "    gr.Markdown(\"Upload audio file below, click **run** to see the output.\")\n",
    "\n",
    "\n",
    "    with gr.Column():\n",
    "        with gr.Row():\n",
    "            audio_inp = gr.File(file_types=['audio'])\n",
    "            audio_out = gr.JSON()\n",
    "        \n",
    "        btn_audio = gr.Button(\"Run stt\")\n",
    "        \n",
    "    \n",
    "    with gr.Column():\n",
    "        gr.Markdown(\"Upload JSON file below, click **run** to see the output.\")\n",
    "        with gr.Row():\n",
    "            text_inp = gr.File(file_types=['.json'])\n",
    "            text_out = gr.JSON()\n",
    "        btn_text = gr.Button(\"Run text inference\")\n",
    "        btn_text.click(fn=run_text_model, inputs=text_inp, outputs=text_out)\n",
    "        \n",
    "    btn_audio.click(fn=run_audio_model, inputs=audio_inp, outputs=[audio_out,text_inp])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 4.19.2, however version 4.29.0 is available, please upgrade.\n",
      "--------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cymn/miniconda3/envs/nvidia/lib/python3.11/site-packages/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/cymn/miniconda3/envs/nvidia/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/cymn/miniconda3/envs/nvidia/lib/python3.11/site-packages/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cymn/miniconda3/envs/nvidia/lib/python3.11/site-packages/gradio/queueing.py\", line 495, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/cymn/miniconda3/envs/nvidia/lib/python3.11/site-packages/gradio/route_utils.py\", line 235, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/cymn/miniconda3/envs/nvidia/lib/python3.11/site-packages/gradio/blocks.py\", line 1627, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/cymn/miniconda3/envs/nvidia/lib/python3.11/site-packages/gradio/blocks.py\", line 1173, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/cymn/miniconda3/envs/nvidia/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/cymn/miniconda3/envs/nvidia/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/cymn/miniconda3/envs/nvidia/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/cymn/miniconda3/envs/nvidia/lib/python3.11/site-packages/gradio/utils.py\", line 690, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_8041/1756527506.py\", line 6, in run_text_model\n",
      "    SOLID_pipeline(input_filepath, output_filepath)\n",
      "  File \"/tmp/ipykernel_8041/845512029.py\", line 12, in SOLID_pipeline\n",
      "    data = data['segments']\n",
      "           ~~~~^^^^^^^^^^^^\n",
      "KeyError: 'segments'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/cymn/miniconda3/envs/nvidia/lib/python3.11/site-packages/gradio/queueing.py\", line 495, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/cymn/miniconda3/envs/nvidia/lib/python3.11/site-packages/gradio/route_utils.py\", line 235, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/cymn/miniconda3/envs/nvidia/lib/python3.11/site-packages/gradio/blocks.py\", line 1627, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/cymn/miniconda3/envs/nvidia/lib/python3.11/site-packages/gradio/blocks.py\", line 1173, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/cymn/miniconda3/envs/nvidia/lib/python3.11/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/cymn/miniconda3/envs/nvidia/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/home/cymn/miniconda3/envs/nvidia/lib/python3.11/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/cymn/miniconda3/envs/nvidia/lib/python3.11/site-packages/gradio/utils.py\", line 690, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_8041/1756527506.py\", line 6, in run_text_model\n",
      "    SOLID_pipeline(input_filepath, output_filepath)\n",
      "  File \"/tmp/ipykernel_8041/845512029.py\", line 12, in SOLID_pipeline\n",
      "    data = data['segments']\n",
      "           ~~~~^^^^^^^^^^^^\n",
      "KeyError: 'segments'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing the dtw module. When using in academic works please cite:\n",
      "  T. Giorgino. Computing and Visualizing Dynamic Time Warping Alignments in R: The dtw Package.\n",
      "  J. Stat. Soft., doi:10.18637/jss.v031.i07.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24089/24089 [02:58<00:00, 134.67frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n",
      "start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cymn/miniconda3/envs/nvidia/lib/python3.11/site-packages/transformers/utils/import_utils.py:517: FutureWarning: `is_compiling` is deprecated. Use `torch.compiler.is_compiling()` instead.\n",
      "  return dynamo.is_compiling()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "interface.launch(share=False)  # Share your demo with just 1 extra parameter ðŸš€\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvidia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
