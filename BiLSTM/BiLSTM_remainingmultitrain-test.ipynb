{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import Counter\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "# Tokenization function\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "# Build vocabulary function\n",
    "def build_vocab(datasets, min_freq=1):\n",
    "    token_counter = Counter()\n",
    "    for texts in datasets:\n",
    "        for text in texts:\n",
    "            tokens = tokenize(text)\n",
    "            token_counter.update(tokens)\n",
    "\n",
    "    # Create vocab with only tokens that meet the minimum frequency\n",
    "    vocab = {token: idx + 2 for idx, (token, freq) in enumerate(token_counter.items()) if freq >= min_freq}\n",
    "    vocab['<PAD>'] = 0\n",
    "    vocab['<UNK>'] = 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "df_olidtest = pd.read_csv('../datasets/cleaned_OLID_test.tsv', sep=\"\\t\")\n",
    "df_solidtest = pd.read_csv('../datasets/cleaned_SOLIDtest6K_trainer.tsv', sep='\\t')\n",
    "df_hso = pd.read_csv('../datasets/cleaned_hatespeech_offensive_test.tsv', sep='\\t')\n",
    "df_troff = pd.read_csv('../datasets/cleaned_tr_offenseval_test.tsv', sep='\\t')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_olid = pd.read_csv('../datasets/cleaned_OLID.tsv', sep='\\t')\n",
    "df_train_solid = pd.read_csv('../datasets/cleaned_SOLID9M_learner.tsv', sep='\\t')\n",
    "# df_train_hso = pd.read_csv('../datasets/cleaned_hatespeech_offensive_train.tsv', sep='\\t')\n",
    "df_train_troff = pd.read_csv('../datasets/cleaned_tr_offenseval.tsv', sep='\\t')\n",
    "\n",
    "df_train_solid['label'] = df_train_solid['average'].apply(lambda x: 1 if x >= 0.8 else 0) # threshold the average values\n",
    "\n",
    "sample_size = 80000\n",
    "positive_ratio = 0.75\n",
    "\n",
    "# Select the most confident positive values\n",
    "semi_tweets_pos_df = df_train_solid[df_train_solid['average'] > 0.8].sample(n=np.floor(sample_size*positive_ratio).astype(int), random_state=1)\n",
    "\n",
    "# Select the most confident negative values\n",
    "semi_tweets_neg_df = df_train_solid[df_train_solid['average'] < 0.2].sample(n=np.floor(sample_size*(1-positive_ratio)).astype(int), random_state=1)\n",
    "\n",
    "semi_tweets_df = pd.concat([semi_tweets_pos_df, semi_tweets_neg_df])\n",
    "semi_tweets_df = semi_tweets_df.sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "semi_tweets_df.rename(columns={'text': 'tweet'}, inplace=True)\n",
    "semi_tweets_df.reset_index(inplace=True, drop=True) \n",
    "\n",
    "df_trainsolidtroff = pd.concat([df_train_troff, semi_tweets_df], ignore_index=True)\n",
    "df_trainsolidtroff = df_trainsolidtroff.sample(frac=1)\n",
    "\n",
    "# df_train_solid = semi_tweets_df\n",
    "\n",
    "\n",
    "datasets = [df_trainsolidtroff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset class\n",
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        tokens = tokenize(text)\n",
    "        text_indices = [self.vocab.get(token, self.vocab['<UNK>']) for token in tokens]\n",
    "        text_indices = text_indices[:self.max_len] + [self.vocab['<PAD>']] * (self.max_len - len(text_indices))\n",
    "        return torch.tensor(text_indices), torch.tensor(label)\n",
    "    \n",
    "\n",
    "# Bi-directional LSTM Model class\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        lstm_out, (hidden, cell) = self.lstm(embedded)\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "        output = self.fc(hidden)\n",
    "        return output\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current training dataset: solid + tr off\n",
      "Epoch: 1, Loss: 0.08728818595409393\n",
      "Epoch: 2, Loss: 0.04023861885070801\n",
      "Epoch: 3, Loss: 0.06003851443529129\n",
      "Epoch: 4, Loss: 0.06022325158119202\n",
      "Epoch: 5, Loss: 0.0019084929954260588\n",
      "Training took: 687.8704490661621\n",
      "[olid] test accuracy: 0.813953488372093\n",
      "test time: 0.40477776527404785\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.93      0.88       620\n",
      "           1       0.75      0.50      0.60       240\n",
      "\n",
      "    accuracy                           0.81       860\n",
      "   macro avg       0.79      0.72      0.74       860\n",
      "weighted avg       0.81      0.81      0.80       860\n",
      "\n",
      "[solid] test accuracy: 0.9050558985483064\n",
      "test time: 2.7342166900634766\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.90      2991\n",
      "           1       0.89      0.93      0.91      3002\n",
      "\n",
      "    accuracy                           0.91      5993\n",
      "   macro avg       0.91      0.91      0.90      5993\n",
      "weighted avg       0.91      0.91      0.90      5993\n",
      "\n",
      "[hso] test accuracy: 0.7831349606616905\n",
      "test time: 2.298262357711792\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.87      0.57       835\n",
      "           1       0.97      0.77      0.85      4122\n",
      "\n",
      "    accuracy                           0.78      4957\n",
      "   macro avg       0.70      0.82      0.71      4957\n",
      "weighted avg       0.88      0.78      0.81      4957\n",
      "\n",
      "[tr_offenseval] test accuracy: 0.8332859174964438\n",
      "test time: 1.6566331386566162\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.94      0.90      2804\n",
      "           1       0.64      0.40      0.49       711\n",
      "\n",
      "    accuracy                           0.83      3515\n",
      "   macro avg       0.75      0.67      0.69      3515\n",
      "weighted avg       0.82      0.83      0.82      3515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_index = 0\n",
    "dataset_names = ['solid + tr off']\n",
    "for train_dataset in datasets:\n",
    "    print(f'current training dataset: {dataset_names[dataset_index]}')\n",
    "    dataset_index += 1\n",
    "    # Build vocabulary\n",
    "    vocab = build_vocab([train_dataset['tweet']], min_freq=1)\n",
    "    \n",
    "    # Parameters\n",
    "    max_len = 512  # Maximum length of text sequences\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_dim = 200\n",
    "    hidden_dim = 256\n",
    "    output_dim = 2  # Number of classes\n",
    "    n_layers = 2\n",
    "    bidirectional = True\n",
    "    dropout = 0.2\n",
    "\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    train_dataset = TextClassificationDataset(train_dataset['tweet'], train_dataset['label'], vocab, max_len)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=120, shuffle=True)\n",
    "\n",
    "    olidtest_dataset = TextClassificationDataset(df_olidtest['tweet'], df_olidtest['label'], vocab, max_len)\n",
    "    olidtest_dataloader = DataLoader(olidtest_dataset, batch_size=120, shuffle=True)\n",
    "\n",
    "    solidtest_dataset = TextClassificationDataset(df_solidtest['tweet'], df_solidtest['label'], vocab, max_len)\n",
    "    solidtest_dataloader = DataLoader(solidtest_dataset, batch_size=120, shuffle=True)\n",
    "\n",
    "    hso_dataset = TextClassificationDataset(df_hso['tweet'], df_hso['label'], vocab, max_len)\n",
    "    hso_dataloader = DataLoader(hso_dataset, batch_size=120, shuffle=True)\n",
    "\n",
    "    troff_dataset = TextClassificationDataset(df_troff['tweet'], df_troff['label'], vocab, max_len)\n",
    "    troff_dataloader = DataLoader(troff_dataset, batch_size=120, shuffle=True)\n",
    "\n",
    "    # Instantiate the model, loss function, and optimizer\n",
    "    model = BiLSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    train_time_start = time.time()\n",
    "    # Training loop\n",
    "    epochs = 5\n",
    "    for epoch in range(epochs):\n",
    "        for inputs, targets in train_dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch: {epoch+1}, Loss: {loss.item()}')\n",
    "    train_time_end = time.time()\n",
    "    print(f'Training took: {train_time_end - train_time_start}')\n",
    "    from sklearn.metrics import classification_report\n",
    "    import numpy as np\n",
    "    import time\n",
    "\n",
    "    # Test function\n",
    "    def test(model, test_dataloader, device):\n",
    "        model.eval()\n",
    "        all_targets = []\n",
    "        all_predictions = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_dataloader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "                all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "        output = classification_report(all_targets, all_predictions)\n",
    "        accuracy = sum(np.array(all_targets) == np.array(all_predictions)) / len(all_targets)\n",
    "        return accuracy, output\n",
    "\n",
    "\n",
    "    test_start_time = time.time()\n",
    "    test_accuracy, test_classification_report = test(model, olidtest_dataloader, device)\n",
    "    test_end_time = time.time()\n",
    "    print(f'[olid] test accuracy: {test_accuracy}')\n",
    "    print(f'test time: {test_end_time - test_start_time}')\n",
    "    print(test_classification_report)\n",
    "\n",
    "\n",
    "    test_start_time = time.time()\n",
    "    test_accuracy, test_classification_report = test(model, solidtest_dataloader, device)\n",
    "    test_end_time = time.time()\n",
    "    print(f'[solid] test accuracy: {test_accuracy}')\n",
    "    print(f'test time: {test_end_time - test_start_time}')\n",
    "    print(test_classification_report)\n",
    "\n",
    "    test_start_time = time.time()\n",
    "    test_accuracy, test_classification_report = test(model, hso_dataloader, device)\n",
    "    test_end_time = time.time()\n",
    "    print(f'[hso] test accuracy: {test_accuracy}')\n",
    "    print(f'test time: {test_end_time - test_start_time}')\n",
    "    print(test_classification_report)\n",
    "\n",
    "    test_start_time = time.time()\n",
    "    test_accuracy, test_classification_report = test(model, troff_dataloader, device)\n",
    "    test_end_time = time.time()\n",
    "    print(f'[tr_offenseval] test accuracy: {test_accuracy}')\n",
    "    print(f'test time: {test_end_time - test_start_time}')\n",
    "    print(test_classification_report)\n",
    "    model.to('cpu')\n",
    "    del model, optimizer, criterion\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nvidia-newest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
